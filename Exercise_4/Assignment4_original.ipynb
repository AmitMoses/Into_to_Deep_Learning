{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment4_original.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV-Baq834KT6"
      },
      "source": [
        "# Assignment 4: Language Processing with RNN-Based Autoencoders\n",
        "\n",
        "**Deadline**: Sunday, June 15th, by 9pm.\n",
        "\n",
        "\n",
        "**Submission**: Submit a PDF export of the completed notebook as well as the ipynb file. \n",
        "\n",
        "In this assignement, we will practice the application of deep learning to natural language processing.\n",
        "We will be working with a subset of Reuters news headlines that are collected over 15 months,\n",
        "covering all of 2019, plus a few months in 2018 and in a few months of this year.\n",
        "\n",
        "In particular, we will be building an **autoencoder** of news headlines. The idea is similar\n",
        "to the kind of image autoencoder we built in lecture: we will have an **encoder** that\n",
        "maps a news headline to a vector embedding, and then a **decoder** that reconstructs\n",
        "the news headline. Both our encoder and decoder networks will be Recurrent Neural Networks,\n",
        "so that you have a chance to practice building\n",
        "\n",
        "- a neural network that takes a sequence as an input\n",
        "- a neural network that generates a sequence as an output\n",
        "\n",
        "This assignment is organized as follows:\n",
        "\n",
        "- Question 1. Exploring the data\n",
        "- Question 2. Building the autoencoder\n",
        "- Question 3. Training the autoencoder using *data augmentation*\n",
        "- Question 4. Analyzing the embeddings (interpolating between headlines)\n",
        "\n",
        "Furthermore, we'll be introducing the idea of **data augmentation** for improving of\n",
        "the robustness of the autoencoder, as proposed by Shen et al [1] in ICML 2020.\n",
        "\n",
        "[1] Shen, Tianxiao, Jonas Mueller, Regina Barzilay, and Tommi Jaakkola. \"Educating text autoencoders: Latent representation guidance via denoising.\" In International Conference on Machine Learning, pp. 8719-8729. PMLR, 2020."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CobqQza4KUC"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        " \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSz5FCNh4KUD"
      },
      "source": [
        "## Question 1. Data (20 %)\n",
        "\n",
        "Download the files `reuters_train.txt` and `reuters_valid.txt`, and upload them to Google Drive.\n",
        "\n",
        "Then, mount Google Drive from your Google Colab notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnABNyg94KUE",
        "outputId": "6577bb0e-d853-46c2-8422-aa8c81115abb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        " \n",
        "train_path = '/content/gdrive/My Drive/Intro_to_Deep_Learning/HW4/data/reuters_train.txt' # Update me\n",
        "valid_path = '/content/gdrive/My Drive/Intro_to_Deep_Learning/HW4/data/reuters_valid.txt' # Update me"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdkp4HLW4KUE"
      },
      "source": [
        "As we did in some of our examples (e.g., training transformers on IMDB reviews) will be using PyTorch's `torchtext` utilities to help us load, process,\n",
        "and batch the data. We'll be using a `TabularDataset` to load our data, which works well on structured\n",
        "CSV data with fixed columns (e.g. a column for the sequence, a column for the label). Our tabular dataset\n",
        "is even simpler: we have no labels, just some text. So, we are treating our data as a table with one field\n",
        "representing our sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2PjMSho4KUF"
      },
      "source": [
        "import torchtext.legacy.data as data\n",
        " \n",
        "# Tokenization function to separate a headline into words\n",
        "def tokenize_headline(headline):\n",
        "    \"\"\"Returns the sequence of words in the string headline. We also\n",
        "    prepend the \"<bos>\" or beginning-of-string token, and append the\n",
        "    \"<eos>\" or end-of-string token to the headline.\n",
        "    \"\"\"\n",
        "    return (\"<bos> \" + headline + \" <eos>\").split()\n",
        " \n",
        "# Data field (column) representing our *text*.\n",
        "text_field = data.Field(\n",
        "    sequential=True,            # this field consists of a sequence\n",
        "    tokenize=tokenize_headline, # how to split sequences into words\n",
        "    include_lengths=True,       # to track the length of sequences, for batching\n",
        "    batch_first=True,           # similar to batch_first=True used in nn.RNN demonstrated in lecture\n",
        "    use_vocab=True)             # to turn each character into an integer index\n",
        "train_data = data.TabularDataset(\n",
        "    path=train_path,                # data file path\n",
        "    format=\"tsv\",                   # fields are separated by a tab\n",
        "    fields=[('title', text_field)]) # list of fields (we have only one)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f20YGOEPXP9Y",
        "outputId": "a3ee2f5f-fb20-4a2f-8c6d-4ded916f14db"
      },
      "source": [
        "print(train_data[0].title)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<bos>', 'dems', 'move', 'to', 'end', 'shutdown', ',', 'without', 'wall', 'money', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TNlqCRM4KUF"
      },
      "source": [
        "### Part (a) -- 5%\n",
        "\n",
        "Draw histograms of the number of words per headline in our training set.\n",
        "Excluding the `<bos>` and `<eos>` tags in your computation.\n",
        "Explain why we would be interested in such histograms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBPd0ezl4KUG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "6919a618-f853-40d9-c9ab-429ecd258d83"
      },
      "source": [
        "# Include your histogram and your written explanations\n",
        " \n",
        "# Here is an example of how to plot a histogram in matplotlib:\n",
        "#plt.hist(np.random.normal(0, 1, 40), bins=20)\n",
        " \n",
        "# Here are some sample code that uses the train_data object:\n",
        "#print(train_data[5].title)\n",
        " \n",
        "num_of_wrds_lis =[]\n",
        "for sentence in train_data:\n",
        "    #print(sentence.title)\n",
        "    #print(len(sentence.title)-2)\n",
        "    num_of_wrds_lis.append(len(sentence.title)-2)\n",
        "    #break\n",
        " \n",
        "plt.hist(num_of_wrds_lis, bins=(max(num_of_wrds_lis)-min(num_of_wrds_lis)))\n",
        "plt.xlabel('Number o f words per headline')\n",
        "plt.ylabel('Number of occurrences')\n",
        "plt.title('Histogram: Number of words per headline')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWnes5SVRwxl"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Data exploration is a process of understanding the data with statistical and visualization methods. This step helps identifying patterns and problems in the dataset.\n",
        "\n",
        "This histogram is helpful to understand the distribution of the number of words for each headline. We shall also create an histogram for the out come of the finished model, to compare the results and evaluate how compressed the model.\n",
        "\n",
        "Amit: to compere the results and evaluate the histogram of the data that the model will generate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm3ysTzh4KUG"
      },
      "source": [
        "### Part (b) -- 5%\n",
        "\n",
        "How many distinct words appear in the training data?\n",
        "Exclude the `<bos>` and `<eos>` tags in your computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEOiE4UGeIt1",
        "outputId": "77912b0a-bef0-4389-a5cf-111f99045a47"
      },
      "source": [
        "# Report your values here. Make sure that you report the actual values,\n",
        "# and not just the code used to get those values\n",
        " \n",
        "# You might find the python class Counter from the collections package useful\n",
        " \n",
        "#import collections\n",
        "from collections import Counter\n",
        " \n",
        "# Create list of all words\n",
        "flt_lst =[]\n",
        "for sentence in train_data:\n",
        "    tmp_snt = sentence.title # Extract the headline\n",
        "    tmp_snt = tmp_snt[1:-1] # Exclude <bos> and <eos>\n",
        "    flt_lst.extend(tmp_snt) # Add to 1 long list\n",
        " \n",
        "# Count and print\n",
        "word_counter = Counter(flt_lst)\n",
        "print('Total number of words: ' + str(len(flt_lst)) + \n",
        "      '\\nTotal number of distinct words: ' + str(len(word_counter)) +\n",
        "      '\\nPercents of distinct: ' + \n",
        "      str(round(len(word_counter)/len(flt_lst)*100,2)) + '%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of words: 1898155\n",
            "Total number of distinct words: 51298\n",
            "Percents of distinct: 2.7%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOeAFZP7eaTp"
      },
      "source": [
        "Track the number of occurrence, of most common unique word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 831
        },
        "id": "H3OiLPw4FKEX",
        "outputId": "b0481f65-a458-4ed4-bd77-db13557b23d0"
      },
      "source": [
        "wrds_num = 30\n",
        "print(str(wrds_num) + ' most common distinct words:')\n",
        "for word, count in word_counter.most_common(wrds_num):\n",
        "    print(word, \": \", count)\n",
        " \n",
        "# Bars plot\n",
        "lst = word_counter.most_common(wrds_num)\n",
        "df = pd.DataFrame(lst, columns = ['Word', 'Count'])\n",
        "df.plot.bar(x='Word',y='Count')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30 most common distinct words:\n",
            "to :  58452\n",
            ", :  43088\n",
            "in :  38538\n",
            "'s :  34580\n",
            "_num_ :  31340\n",
            ": :  28052\n",
            "on :  24794\n",
            "of :  22895\n",
            "for :  22163\n",
            "u.s. :  19320\n",
            "update :  18167\n",
            "as :  17167\n",
            "says :  14089\n",
            "- :  12525\n",
            "with :  10552\n",
            "after :  9913\n",
            "china :  9724\n",
            "at :  8524\n",
            "trump :  8229\n",
            "$ :  8176\n",
            "trade :  7898\n",
            "new :  7512\n",
            "over :  7040\n",
            "from :  6717\n",
            "deal :  6360\n",
            "and :  6279\n",
            "oil :  5536\n",
            "by :  5352\n",
            "the :  5074\n",
            "a :  4761\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEfCAYAAACjwKoqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3debhcVZ3u8e9LCAQkkAAx0ARIVBAhGEgCRGalhSAqSCPKRUkjQqOg0A4ttveKgrZi24Jgg40QCDgwCDRRgzFGkEkgCUOYmzSCHGSIJAyCRAO/+8dalexUdp3adU6dKef9PE89p/aqVatW1dlVv72GvbYiAjMzG9zW6usKmJlZ33MwMDMzBwMzM3MwMDMzHAzMzAxYu68r0FWbbrppjB07tq+rYWY2YCxYsOBPETGq7LEBGwzGjh3L/Pnz+7oaZmYDhqTHGz1WqZtI0ghJP5X0kKQHJb1D0saS5kh6JP8dmfNK0tmSFklaKGlioZxpOf8jkqYV0idJujc/52xJ6s4bNjOz1lQdM/gu8MuI2A6YADwInALMjYhtgLl5G+BAYJt8Ow44D0DSxsCpwG7ArsCptQCS8xxbeN7U7r0tMzNrRdNgIGkjYG/gQoCI+GtEPA8cDMzI2WYAh+T7BwOXRHIbMELS5sABwJyIWBIRS4E5wNT82IYRcVuk06EvKZRlZma9oMqYwThgMXCRpAnAAuAkYHREPJXzPA2Mzve3AJ4oPL8jp3WW3lGSvhpJx5FaG2y11VYVqm5mg9Hf/vY3Ojo6ePXVV/u6Kn1i2LBhjBkzhqFDh1Z+TpVgsDYwEfhURNwu6bus7BICICJCUo8vchQR5wPnA0yePNmLKplZqY6ODoYPH87YsWMZbEOQEcFzzz1HR0cH48aNq/y8KmMGHUBHRNyet39KCg7P5C4e8t9n8+NPAlsWnj8mp3WWPqYk3cysS1599VU22WSTQRcIACSxySabtNwqahoMIuJp4AlJb81J+wEPADOB2oygacC1+f5M4Kg8q2gK8ELuTpoN7C9pZB443h+YnR97UdKUPIvoqEJZZmZdMhgDQU1X3nvV8ww+BfxI0jrAo8DRpEByhaRjgMeBw3PeWcB7gEXAKzkvEbFE0unAvJzvtIhYku9/ErgYWA+4Lt/MzKyXVAoGEXE3MLnkof1K8gZwQoNypgPTS9LnA+Or1MXMrFVjT/lFW8t77JsHNc3z9NNPc/LJJzNv3jxGjBjB6NGjOeuss9h2223bUocbbriBddZZh913370t5Q3YM5BrGv2Tq/yzzMx6QkTwgQ98gGnTpnHZZZcBcM899/DMM8+0NRhssMEGbQsGXqjOzKzNrr/+eoYOHcrxxx+/Im3ChAnsueeefP7zn2f8+PHsuOOOXH755UD6YX/ve9+7Iu+JJ57IxRdfDKSld0499VQmTpzIjjvuyEMPPcRjjz3G97//fc4880x22mknbrrppm7XecC3DMzM+pv77ruPSZMmrZZ+9dVXc/fdd3PPPffwpz/9iV122YW99967aXmbbropd955J+eeey7f/va3ueCCCzj++OPZYIMN+NznPteWOrtlYGbWS26++WaOOOIIhgwZwujRo9lnn32YN29e0+cdeuihAEyaNInHHnusR+rmYGBm1mY77LADCxYsqJx/7bXX5vXXX1+xXX+OwLrrrgvAkCFDWL58eXsqWcfBwMyszd71rnexbNkyzj///BVpCxcuZMSIEVx++eW89tprLF68mBtvvJFdd92VrbfemgceeIBly5bx/PPPM3fu3KavMXz4cF566aW21dljBma2xuvt2YWSuOaaazj55JM544wzGDZsGGPHjuWss87iz3/+MxMmTEAS3/rWt9hss80AOPzwwxk/fjzjxo1j5513bvoa73vf+zjssMO49tprOeecc9hrr726V+d0WsDAM3ny5Jg/f76nlprZah588EHe9ra39XU1+lTZZyBpQUSUnTPmbiIzM3MwMDMzHAzMbA01ULvA26Er793BwMzWOMOGDeO5554blAGhdj2DYcOGtfQ8zyYyszXOmDFj6OjoYPHixX1dlT5Ru9JZKxwMzGyNM3To0Jau8mXuJjIzMxwMzMwMBwMzM8PBwMzMcDAwMzMcDMzMDAcDMzPDwcDMzHAwMDMzHAzMzAwHAzMzw8HAzMyoGAwkPSbpXkl3S5qf0zaWNEfSI/nvyJwuSWdLWiRpoaSJhXKm5fyPSJpWSJ+Uy1+Un6t2v1EzM2uslZbBOyNip8L1M08B5kbENsDcvA1wILBNvh0HnAcpeACnArsBuwKn1gJIznNs4XlTu/yOzMysZd3pJjoYmJHvzwAOKaRfEsltwAhJmwMHAHMiYklELAXmAFPzYxtGxG2RrkRxSaEsMzPrBVWDQQC/krRA0nE5bXREPJXvPw2Mzve3AJ4oPLcjp3WW3lGSbmZmvaTqxW32jIgnJb0RmCPpoeKDERGSevz6cjkQHQew1VZb9fTLmZkNGpVaBhHxZP77LHANqc//mdzFQ/77bM7+JLBl4eljclpn6WNK0svqcX5ETI6IyaNGjapSdTMzq6BpMJD0BknDa/eB/YH7gJlAbUbQNODafH8mcFSeVTQFeCF3J80G9pc0Mg8c7w/Mzo+9KGlKnkV0VKEsMzPrBVW6iUYD1+TZnmsDP46IX0qaB1wh6RjgceDwnH8W8B5gEfAKcDRARCyRdDowL+c7LSKW5PufBC4G1gOuyzczM+slTYNBRDwKTChJfw7YryQ9gBMalDUdmF6SPh8YX6G+ZmbWA3wGspmZORiYmZmDgZmZ4WBgZmY4GJiZGQ4GZmaGg4GZmeFgYGZmOBiYmRkOBmZmhoOBmZnhYGBmZjgYmJkZDgZmZoaDgZmZ4WBgZmY4GJiZGQ4GZmaGg4GZmVHhGshrmrGn/GK1tMe+eVAf1MTMrP9wy8DMzBwMzMzMwcDMzHAwMDMzHAzMzAwHAzMzw8HAzMxoIRhIGiLpLkk/z9vjJN0uaZGkyyWtk9PXzduL8uNjC2V8Mac/LOmAQvrUnLZI0inte3tmZlZFKy2Dk4AHC9tnAGdGxFuApcAxOf0YYGlOPzPnQ9L2wIeBHYCpwLk5wAwB/hM4ENgeOCLnNTOzXlIpGEgaAxwEXJC3BbwL+GnOMgM4JN8/OG+TH98v5z8YuCwilkXE74FFwK75tigiHo2IvwKX5bxmZtZLqrYMzgL+BXg9b28CPB8Ry/N2B7BFvr8F8ARAfvyFnH9Fet1zGqWvRtJxkuZLmr948eKKVTczs2aaBgNJ7wWejYgFvVCfTkXE+RExOSImjxo1qq+rY2a2xqiyUN0ewPslvQcYBmwIfBcYIWntfPQ/Bngy538S2BLokLQ2sBHwXCG9pvicRulmZtYLmrYMIuKLETEmIsaSBoB/ExFHAtcDh+Vs04Br8/2ZeZv8+G8iInL6h/Nso3HANsAdwDxgmzw7aZ38GjPb8u7MzKyS7ixh/QXgMklfA+4CLszpFwKXSloELCH9uBMR90u6AngAWA6cEBGvAUg6EZgNDAGmR8T93aiXmZm1qKVgEBE3ADfk+4+SZgLV53kV+GCD538d+HpJ+ixgVit16Q2+9oGZDRY+A9nMzBwMzMzMwcDMzHAwMDMzHAzMzAwHAzMzw8HAzMxwMDAzMxwMzMwMBwMzM8PBwMzMcDAwMzMcDMzMjO4tYW0FXuHUzAYytwzMzMzBwMzMHAzMzAwHAzMzw8HAzMxwMDAzMxwMzMwMBwMzM8PBwMzMcDAwMzMcDMzMDK9N1CfK1jECr2VkZn2nactA0jBJd0i6R9L9kr6a08dJul3SIkmXS1onp6+btxflx8cWyvpiTn9Y0gGF9Kk5bZGkU9r/Ns3MrDNVuomWAe+KiAnATsBUSVOAM4AzI+ItwFLgmJz/GGBpTj8z50PS9sCHgR2AqcC5koZIGgL8J3AgsD1wRM5rZma9pGkwiOTPeXNovgXwLuCnOX0GcEi+f3DeJj++nyTl9MsiYllE/B5YBOyab4si4tGI+CtwWc5rZma9pNIAcj6Cvxt4FpgD/C/wfEQsz1k6gC3y/S2AJwDy4y8AmxTT657TKL2sHsdJmi9p/uLFi6tU3czMKqgUDCLitYjYCRhDOpLfrkdr1bge50fE5IiYPGrUqL6ogpnZGqmlqaUR8TxwPfAOYISk2mykMcCT+f6TwJYA+fGNgOeK6XXPaZRuZma9pMpsolGSRuT76wHvBh4kBYXDcrZpwLX5/sy8TX78NxEROf3DebbROGAb4A5gHrBNnp20DmmQeWY73pyZmVVT5TyDzYEZedbPWsAVEfFzSQ8Al0n6GnAXcGHOfyFwqaRFwBLSjzsRcb+kK4AHgOXACRHxGoCkE4HZwBBgekTc37Z3aGZmTTUNBhGxENi5JP1R0vhBffqrwAcblPV14Osl6bOAWRXqa2ZmPcDLUZiZmZej6O+8dIWZ9Qa3DMzMzMHAzMwcDMzMDAcDMzPDwcDMzHAwMDMzHAzMzAwHAzMzw8HAzMxwMDAzMxwMzMwMr020RvE6RmbWVW4ZmJmZg4GZmTkYmJkZDgZmZoaDgZmZ4WBgZmZ4aumg5WmoZlbkloGZmbllYNWUtSTcijBbc7hlYGZmDgZmZuZgYGZmOBiYmRkVgoGkLSVdL+kBSfdLOimnbyxpjqRH8t+ROV2Szpa0SNJCSRMLZU3L+R+RNK2QPknSvfk5Z0tST7xZMzMrV6VlsBz4bERsD0wBTpC0PXAKMDcitgHm5m2AA4Ft8u044DxIwQM4FdgN2BU4tRZAcp5jC8+b2v23ZmZmVTUNBhHxVETcme+/BDwIbAEcDMzI2WYAh+T7BwOXRHIbMELS5sABwJyIWBIRS4E5wNT82IYRcVtEBHBJoSwzM+sFLY0ZSBoL7AzcDoyOiKfyQ08Do/P9LYAnCk/ryGmdpXeUpJe9/nGS5kuav3jx4laqbmZmnagcDCRtAFwFnBwRLxYfy0f00ea6rSYizo+IyRExedSoUT39cmZmg0alM5AlDSUFgh9FxNU5+RlJm0fEU7mr59mc/iSwZeHpY3Lak8C+dek35PQxJfltgPLZymYDT5XZRAIuBB6MiO8UHpoJ1GYETQOuLaQflWcVTQFeyN1Js4H9JY3MA8f7A7PzYy9KmpJf66hCWWZm1guqtAz2AD4K3Cvp7pz2r8A3gSskHQM8DhyeH5sFvAdYBLwCHA0QEUsknQ7My/lOi4gl+f4ngYuB9YDr8s3MzHpJ02AQETcDjeb971eSP4ATGpQ1HZhekj4fGN+sLmZm1jO8aqn1KV9Xwax/8HIUZmbmYGBmZg4GZmaGxwxsAPH4glnPccvAzMwcDMzMzMHAzMxwMDAzMxwMzMwMBwMzM8PBwMzMcDAwMzMcDMzMDAcDMzPDy1HYGspLV5i1xi0DMzNzMDAzMwcDMzPDwcDMzPAAspkHm81wy8DMzHAwMDMzHAzMzAwHAzMzw8HAzMyoEAwkTZf0rKT7CmkbS5oj6ZH8d2ROl6SzJS2StFDSxMJzpuX8j0iaVkifJOne/JyzJandb9LMzDpXZWrpxcD3gEsKaacAcyPim5JOydtfAA4Etsm33YDzgN0kbQycCkwGAlggaWZELM15jgVuB2YBU4Hruv/WzHpG2VRUT0O1ga5pyyAibgSW1CUfDMzI92cAhxTSL4nkNmCEpM2BA4A5EbEkB4A5wNT82IYRcVtEBCngHIKZmfWqro4ZjI6Ip/L9p4HR+f4WwBOFfB05rbP0jpL0UpKOkzRf0vzFixd3sepmZlav22cgR0RIinZUpsJrnQ+cDzB58uReeU2z7milS8lnQltf6mrL4JncxUP++2xOfxLYspBvTE7rLH1MSbqZmfWirgaDmUBtRtA04NpC+lF5VtEU4IXcnTQb2F/SyDzzaH9gdn7sRUlT8iyiowplmZlZL2naTSTpJ8C+wKaSOkizgr4JXCHpGOBx4PCcfRbwHmAR8ApwNEBELJF0OjAv5zstImqD0p8kzVhajzSLyDOJzMx6WdNgEBFHNHhov5K8AZzQoJzpwPSS9PnA+Gb1MLOVPL5g7eYzkM3MzNczMFvTtdKKcItj8HIwMLMu89nYaw4HAzPrFQ4c/ZvHDMzMzC0DM+t/3IrofQ4GZjagedC7PRwMzGzQ8MyqxjxmYGZmDgZmZuZuIjOzthjog94OBmZmvaw/Bg4HAzOzfqy3AoeDgZnZGqI7M6A8gGxmZg4GZmbmYGBmZjgYmJkZDgZmZoaDgZmZ4WBgZmY4GJiZGQ4GZmaGg4GZmeFgYGZmOBiYmRkOBmZmRj8KBpKmSnpY0iJJp/R1fczMBpN+EQwkDQH+EzgQ2B44QtL2fVsrM7PBo18EA2BXYFFEPBoRfwUuAw7u4zqZmQ0aioi+rgOSDgOmRsTH8/ZHgd0i4sS6fMcBx+XNtwIP1xW1KfCnii/bU3n7Sz36Q97+Uo/+kLe/1KM/5O0v9RhoedtR9tYRMao0d0T0+Q04DLigsP1R4HtdKGd+X+ftL/XoD3n7Sz36Q97+Uo/+kLe/1GOg5e3psvtLN9GTwJaF7TE5zczMekF/CQbzgG0kjZO0DvBhYGYf18nMbNBYu68rABARyyWdCMwGhgDTI+L+LhR1fj/I21/q0R/y9pd69Ie8/aUe/SFvf6nHQMvbo2X3iwFkMzPrW/2lm8jMzPqQg4GZmTkY9DeSpvV1HQYzSZv1dR3M+sKgDgaSNpe0bl/Xo85JPVm4pEvz30qvI2mIpB/1ZJ36mVntKGSgfm6S3ijpDfn+epK+JOmbkjbv5DnrVyj3zRVfX5K2bJ7TykgaKWlXSXvXblWf2y9mE3WHpNHALnnzjoh4toWnXwq8WdJVEfG5krJ3B8ZS+Jwi4pIG9dgDuDsiXpb0EWAi8N2IeLyF+gCopOzNIuLpRtuF9HWBfyip82mFbJMk/R3wMUmX1L9eRCyp235N0taS1om0VEhbSRJwJPCmiDhN0lbAZhFxR0nek4CLgJeAC4CdgVMi4lcleb8FfA34C/BL4O3AP0fED5tVqUKdm+4XXfncJI0nrc01rFG5Od+lEfHRZmk5fRjwSWBPIICbgfMi4tUG1bgM+EfgZeCrwCjgIeDHwDvryt6d9H/YANhK0gTgnyLikyXlTpc0hjSN/Cbgxoi4tz5TRISkWcCODeq3GklvZ/X/x9WFx88hvfdSEfHpqq9V8tr3Nihbqeh4e8lzrgYuBK6LiNeblD8a+Dfg7yLiwLxm2zsi4sKSvB8nHUyOAe4GpgC/A95V5b0M6GAg6XDg34EbSB/+OZI+HxE/rfL8iPj7/GO02qJ4+Qj6zaQP9bXaU4DSYACcB0zIX4jPkr4klwD7VH5DK1+j3oXAQZ1s11wLvAAsAJY1KP/7wFzgTTlfjfJrv6nkOY8Ct0iaSfqRSBWN+E6D11iFpDsjYmKDh88FXiftsKeRfuivYmWAL/pYRHxX0gHASNKZ6pcCqwUDYP+I+BdJHwAeAw4FbgSaBYMfNHkvrewXlT83SacC+5L2xVmkRRtvblDuDnXPXRuY1KDKl5A+03Py9v8hfWYfLKnDtPze9s3fiw8B3wL+DGwt6SjSAc/C/JQzgQPI5wRFxD2NjkQjYp98DtEu+X3+QtIGEbFxSfY7Je0SEfMavKdinaeTAv39pP0I0v/j6kK2+c3KqSvzJToPHhsWNt/bStnZucDRwNmSrgQuioj6pXVqLiYdAH0pb/8PcDnpN6DeSaTP97aIeKek7UiBpJIBHQxIH9AutdaApFHAr4FKwQBS6CbtSPUmA9tH9bm3y/NRzcGkpTQulHRM1XoUrHZkGhEHdbZdMCYipnZWeEScTdoJzyMFhtqX98aIuKfB0/4339YChndWfoPXbBQIIK1BNVHSXTnv0vyjUab22bwHuDQi7s8/WmVq+/ZBwJUR8ULjrKvU9dwmWVrZL1r53A4DJgB3RcTR+YhwlcAl6YvAvwLrSXqx8NDfaDynfHxEFA92rpf0QIO8N5CC1kJgE+AZ4Gekz/2E/PgLxSdExBN1n+trlJC0J7BXvo0Afk5qIZTZDThS0uO5Pg2PsoEpde9vNRExo7PHS/IPz3U+HXiKFDxrLdjN6/K22vInIn4N/FrSRsAR+f4TpAORH0bE3wrZN42IK/L/vnZOVulnDLwaEa9KQtK6EfGQpLdWrddADwZr1XULPUf7xkHuAzYj7QxVvJT/YR8B9pa0FjC0C697SxeeU3OrpB3Lmt8lHiL92FxN2tEvlfSDiDinPmNEfBVA0gZ5+8+NCs39zX+JiNclbQtsR2oO/63BU/6mtIR55OePYuURXr0Fkn4FjAO+KGl4J3l/LukhUjfRJ3K5jbpGWlF5vyh8butHxCtNstc+s+WSNgSeZdUlWoiIbwDfkPQN0hH7tqzsUmoUnO6UNCUibst12Y0GR8oR8XjuUplN+lyPjYg/5K675yLiD3VPeSJ3FYWkoaQj0wcb1OMGUkv0G8CsJl1nB3TyWL3fSdo+IhoFuBXyPvAFVu+Ka9SN8v6ImFDYPk/SPcCXC2XeHBF7lrQmagGs2Ioo1mUTUsv2I8BdwI9IXXnTSC2nmpdz3tr3Ywp1AbmgQ9II4L+BOZKWAtWDVSsLGfW3G+kLMZvUx/mPwHXAGW0q+3pgaS5/Zu3WSf7NgM8Ae+XtrYCjSvKNAD4NfAc4u3ZrU50fIB0lPkw6ursXWNgg70LgDYXtN3SSd3zeYR/PtwXADg3yLgDWB7Ygdc9cCfyokzofmT/bDuDrue6HN8i7FmksZjdSi+ZQ4FOdlL0xMKTw/jbrxmf7s1zPyvsF8I78P/lD3p4AnNsg77l53zgeeCR/3hc1yHts/t8uzfX5C/CbBnkfJP2wP5Zvr+e0zvaNDYD16/aNESX5NiX9iD1DCl4/BDZpUOYIUivtDOA3pBb86Z183nsCR+f7o4BxDfLtQ/pxrLLP/wo4Jr//fYDpdPJ7Adya988hed87Eri1k/w7AZ/Ktwmd5Lsm7xdfrN8nqVtcLu/vt+T3eAupm+jtFfbXfYD3A+tU3ccH9BnIks4AbiftOJCanVMi4gttKLu0rz8iftvNcm8FbiPttCuOaqPFpmyDsrcm9aXvlZNuBJ6PkqZsHvjaJfJAYh5onBcRqw3c5Tp/KSKuz9v7Av8WEbuX5L0zUrfPp4D1IuJbku6OiJ06qfd2wH6ko6m5EVF6dNlogCxKjuwkLSD1q/4kIpY2eu2qGu0PNWX7haTbSd0/MyNi55x2X0SMb/JaY4ENY2XffP3j97Kyb3inWt9wRBxaknfrJvVuuZujqyS9jfQjtRewOylIrva55vGTycBbI2JbpQkPV0bEHiV5F5EOwuq/T2X7/IKImCRpYeQuJ0nzIqJsfKr2f/gusAfpyPwW4OSIeKwk76dJQbrW0j4EKG1pSzqQNO6zR65zp4P6eUzorbnch6NxK7tbBno30bvzD39x5sBXSU3Bbqn6oy/p96QdZXFE7FbhKcMi4jPdqlxjhwAfp9D1Q+qHXG2HJA1K3S7pmsJzywalILUgrq9tRMQNuTuojCS9g3QUVRszGdKowlo5C+ahkrR6rQyQfYg0SDdP0nzS+/1VdPHop7Y/SDqj/mAjH5SU7i9RsU89l7MFsDX5eylp74i4sSRr5b7hSF0/I0ldTsXZNnc2qkcz6sLsHEmPkv7HN5MmWxwdjbuKPkCaKXZnLu+PuUuwzOKIqLqoZe1H9ClJBwF/JLUeS+Uf/aoX2fo46UD0ZVixT/yO8u/e0cCLpF4B6GRQP9uVlbOlJkoiGsxq7I4BGQwkfYI0Xe5NkopHT8PpXp97y32AETGuxZe4VNKxpAG0FTN+om5KZxcdQ8UdMiK+I+kGVraqjo6IuxqU+6ik/0faYSH1cz7aIO9JpObvNZEGeN9E6spopH5mzBAaz4xp5UdwEfClXO/3kroEXpN0EWnKb1c/73ez+sHGgSVp0EKfev5ffYjUfVCcpVQWDCr3DedB0H8kDWTX9ueg4nTDBmpjDnuQ+t8vz9sfzPUv85ZoMo2y4K8REZJq/eSNDjwA7pL0Y1I3XvH7dHVJ3q/lQdvPkr4TGwL/3KjgPMZwLKtPW/1YWXZWDfSv0XiacuVBfbU+q7HLBmQwIM15vo40GHVKIf2l7v6oRsSe+W/Ls2Yq+itpOuyXWPXLWTals1Wt7JC1o8OGR4iFI/SbSF+I2hfsRqDsC0E+kr2xsP0oaYykvuz6mTG1ev6VxjNjWhogU5p/fjRp9tFVrByk+w2pf7eyLh6AHE/qZtiCdH2OX+UyyhxC6hZpNCV4hYj4QL77FUnXAxuRzqUoczjw5k6OwltW69LMn8meEbE8b3+fxjOE3qI0g210RIzP/5v3R8TXSvJeIem/gBH5wOljNJ7yux4pCOxfrCKrTi2t1fvn+e4L1J0z0cC1pPfzazpp0WWttLQrD+rT+qzGrqs6uOBbe26kI+pNe6jszwD3AF/Jt7tJfZxdLe8B4O9ymRuTphtuXLu1UM5xnTz2jS7WrdMBMtJA9lxSE3zduseu7sLrbUQKiD8hdeXUbg0/B2CPKmk5/Tpggx7YJ64C3thD+9vDxfdPGq96uEHe35K6O+4qpN3XSdnvJh00fZvUHdyO+m6b94n78vbbgf/bSf67Wyx/IunA59PAziWP30sa5C4O6v8+33+gQZlXApv3xP+v/jagB5AHIqWpkYdE86mGXS1/IoUB9Wjc9VOlrE8DnyC1WopXnqt1mVVqzUj6p4j4r04eHwlsw6rT/cq6RyqT9KZIrZK2kLRhRLwoqbSPOUpapCo52a4sLadfRZptNJdVuzu6fHZsLncy6Qj3vrpy39+dcnPZR5MOOq4n7RN7A1+JkskQtYFaSXfFysH00okFkj4DXB4RTa92qHRW8zmkLitIR/InRURHSd7fAp8H/isqDOhL+hpp9lC7liipPJgv6WekFs5wUiv2Dtr8/6s3ULuJBrKXgbtz875tX/pCOZ12/bRY1ooT1CLiE1WeI+nLzXOtkr9bp9A3EhGP5kHCHVg1yJzW+Fmd+jFp7GEB6Uta7H5bpZsvD6DvDozKP2w1G9J4ML02TbXdZpCmc64y26YdIuIiSdeRpvoCfCFKlknJ/qS0PlFtHFdWgOsAAAjPSURBVOAwGp+rMRz4laQlpPGIKyPimQZ5LyL9b2qDrx/Jae8uybt+RNxRN6C/vEG5kPbLf5W0jDT43Om5A81EazO3vp1f7wxSl1NNLa3tHAx633/n24BRNRBkLxfuDyP9gDY6EQm6eQp9I7n/en1S3/AFpCmeq613VFVE1JYduIXU5XFTRDzUIPtQ0lz9tVn1zOMXcz3Kyu/21OIGXslBvacsI/2oDwO2lbRtg1bdCaSxoO0kPUnqHjmyrMBIJ+t9NY8rfAj4raSOiPj7kuyjIuKiwvbFkk5uUNdWAhIRMTy3BFdptfaGWDl7bWjUzWyUtF5PvKa7iaxHKS2eNzsi9m3weK374G7S0hTLJN0fETuU5W/hdRdGxNsLfzcgnQm9V9Mnd17uO1m5rMKbSa2wmyLiu4U8cyNiP0lXRMThTcq7IiIOV4MFz6J8CYZW6vsd0g/2TFZtiXa79dioVRfl532sSwqEY0ljTi+majRuqSktJ/5B0jXRh5d9FpLmkloCP8lJR5Bmxu1XkvdNpIC0O+mEvd8DRzY6Ym/w/m4tK7vdihMWSDPBaoYDt0TER9r9mm4Z9DKtPC9hFVX73weg9Ulfpka6dwp9Y7UTeF5ROmlpCXXrynRFRFwv6UZSa+adpBlD40mzhmo2z1NKd5S0M3Uzuup+iGtLiXdlwbMqds5/pxSrQDe74bJWWnXXAs+TgucfOytU0idJs6BGkQZQj43Gy018jDRmcCbpfd1KmkFWLK/YVTeLNMaxFqkV+w+k1QC6+/7arcdmTDbiYND7JhfuDyMd+TQ88WWgqTvCHUL6Qjc8+ovWpkm24mc5yPw76QcoaLIiaRX5SPQNpHGNmygslFjwZeD/kYJg/Q/NKj/EEfFU/tsjZwJHRJUplF3VysJoTRdRLNiS9EO8N+nzKl3jK5+T8m8VBlNrXXVvJf24X0sK0B+l867Dbi381h0R8QJpCuwRvfF64GDQ6yLiubqks5SWTmhp4LUfKx7hLgeeiTwPvajBrJzaAnsbkI7ku+Mh4LWIuEppDfiJtGesZiHppLjxpC/r85J+FxF/qWWItIT6T5VOePseKxeUa9gnK+lQ0sDgG0k/VN0arCyUW7pfdWMgvaiVVl0riyg+zaqLKP5Q0vlRt7RDVLxmRKxcMPBGYGJEvJS3vwL8ok3vb8DzmEEvy1M/a9YitRQ+EauujrjGK3SXibSo39J8fwRpzZpWz+yuL782VrAncDppdsaXo9qSIVXKH046s/dzpMXGVrtiXj5h6tNU6HNWWmPnfdFgXaZu1POzhc0VA/pRfhZtd15nH3KrruyHWekM27eQ+umXQeNlqZVO6ntHrDyT/g2ksYiyvJcAbyONiTS7ZsTDpEXeluXtdUmL2jU92m/2/tYEbhn0vv8o3F9OOvGk00HGNVHtx17SD0hLV8zK2wey6lS6rqqdMXoQacGwX+R5490i6UTS4PEk0v9uOo3Puv001fucn2l3IACIiOL+hqRvk1Zc7ZbcRXN/RGyXX6fZWl4HtlI8Tc6k18qz499PGi+ocs2IS4A7tOpZwhdXqVCF9zfgORj0sh7uwx2IpkTEsbWNiLhO6ZKV3fWk0pIG7wbOyEeB7bjWxTDSOMCCsu6vOk37nHP3EMB8SZeTuiSarbHTHc0G9CvJXTQPS9oqVr/OQVn+VrpXqiztULt86x8oXwyurA5fz+dF1GaUdbYe16DjbqJepmrXKR40JM0mHVnXrup1JLB3RLRygZOyctcHpgL3RsQjShd03zFKrpfcU/KP2dHAyaRB46XA0Ih4TyFPbY58/YlskLpRutWd02hAPyK+151yc9k3kmYr3cGqXTTtOLu50zPptfLs+HGsOjuppbPjbSUHg14m6ZesvE7xiqZwfXN+sMgDyadSuPwm8NWemj7XVyr0qc8gLaPwfN4eCfxHG4JBcQmEhgP6XSz7DtLyDiuSSBeLacu4TMU6VD473jrnYNDLVOHiJoOR0tLCr9dmegw2KqzZ01lai2Wu0q/fbipfe2nFhWNsYGnX9YKtulslrXY1scFK0i65K+Me4F5J90hqdD2DNdlauTUArGgxdWtMLyJeAx5WuoZx20j6RP6fvVXSwsLt96SptzYAuWXQy5pNsZM0MtpwmcaBIk8jPCEibsrbe5KuEzyoji4lHUW6vsOVOemDwNcj4tLGz6pUbtv79XMrbiS9eHas9TwHg16mBsvY1mZblDW912QNukcG1WdQk0+Oq52d/JtOlmBopcw+79e3gcFTS3tZhSl2Da9Mtob6bZ4C+hPSrJcPATfUTs6LNiyoNlDkH/9uB4A6a9fPkVcPrXppA5uDQf8z2JpqtTOva8smiPQZ7Ez7FlQbdNSD1wm3NZODgfW1G+q2AwbveRdt1OurXtrA5mDQ/wy2bqI/F+5XuRiOVdAXq17awOYB5H5G0saD+chNTS6GY2Y9w+cZ9DODORBkbVk7x8xa424i61OtXgzHzHqGu4msT/Xk2jlmVp2DgZmZeczAzMwcDMzMDAcDs05JOlPSyYXt2ZIuKGz/h6TPdKHcfSX9vF31NOsuBwOzzt0C7A4gaS1gU2CHwuO7A7c2KyRfW8Cs33IwMOvcrcA78v0dgPuAlySNzCfIvQ3YSNJdku6VND2nI+kxSWdIuhP4oKSpkh7K24eWvZhZX3EwMOtERPwRWJ4vELM78DvgdlKAmAw8AlwAfCgidiSdu1O8DONzeTnu/wZ+ALwPmARs1mtvwqwCBwOz5m4lBYJaMPhdYbsD+H1E/E/OO4OV13MGuDz/3S7neyTSfO4f9kbFzapyMDBrrjZusCOpm+g2Ustgd1ZfdbXey00eN+sXHAzMmruVtJrqkoh4La8fNYIUEK4Cxkp6S877UeC3JWU8lPO9OW97NVHrVxwMzJq7lzSL6La6tBciogM4Grgyr7P0OvD9+gIi4lXgOOAXeQD52R6vtVkLvByFmZm5ZWBmZg4GZmaGg4GZmeFgYGZmOBiYmRkOBmZmhoOBmZkB/x91ttD2aqRsgQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5lgEwiM4KUH"
      },
      "source": [
        "### Part (c) -- 5%\n",
        "\n",
        "The distribution of *words* will have a long tail, meaning that there are some words\n",
        "that will appear very often, and many words that will appear infrequently. How many words\n",
        "appear exactly once in the training set? Exactly twice?\n",
        "Print these numbers below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpkUDVoC4KUH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d1f3469-7f3f-4168-8c6c-c1412064486a"
      },
      "source": [
        "# Report your values here. Make sure that you report the actual values,\n",
        "# and not just the code used to get those values\n",
        " \n",
        "wrds_num = len(word_counter) # Chose all words\n",
        "appear_once = 0 # Reset counter\n",
        "appear_twice = 0 # Reset counter\n",
        "for word, count in word_counter.most_common()[:-wrds_num-1:-1]: # .most_common()[:-wrds_num-1:-1] is necessary, since just word_counter wont work\n",
        "    if count == 1:\n",
        "        appear_once += 1\n",
        "    elif count == 2:\n",
        "        appear_twice += 1\n",
        "    #print(word, \": \", count)\n",
        " \n",
        "print('Number of words that appears once: ' + str(appear_once) + ' (' +\n",
        "      str(round(appear_once/wrds_num*100,2)) + '% out of distinct words)' +\n",
        "      '\\nNumber of words that appears twice: ' + str(appear_twice) + ' (' +\n",
        "      str(round(appear_twice/wrds_num*100,2)) + '% out of distinct words)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words that appears once: 19854 (38.7% out of distinct words)\n",
            "Number of words that appears twice: 7193 (14.02% out of distinct words)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3RKPwdI4KUI"
      },
      "source": [
        "### Part (d) -- 5%\n",
        "We will replace the infrequent\n",
        "words with an `<unk>` tag, instead of learning embeddings for these rare words. `torchtext` also provides us with the `<pad>` tag used for padding short sequences for batching.\n",
        "We will thus only model the top 9995 words in the training set, excluding the tags\n",
        "`<bos>`, `<eos>`, `<unk>`, and `<pad>`.\n",
        "\n",
        "What percentage of total word count(whole dataset) will be supported? Alternatively, what percentage\n",
        "of total word count(whole dataset) in the training set will be set to the `<unk>` tag?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cva5DC_64KUI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8924c836-e8ed-4218-a4a1-d1d9a7813858"
      },
      "source": [
        "# Report your values here. Make sure that you report the actual values,\n",
        "# and not just the code used to get those values\n",
        " \n",
        "infrequent_words = appear_once + appear_twice*2\n",
        " \n",
        "# Create list of all words\n",
        "flt_lst =[]\n",
        "for sentence in train_data:\n",
        "    tmp_snt = sentence.title # Extract the headline\n",
        "    #tmp_snt = tmp_snt[1:-1] # Exclude <bos> and <eos>\n",
        "    flt_lst.extend(tmp_snt) # Add to 1 long list\n",
        " \n",
        "per = round(infrequent_words/len(flt_lst)*100,2)\n",
        "print('Total words: ' + str(len(flt_lst)) + \n",
        "      '\\nInfrequent words: ' + str(infrequent_words) +\n",
        "      '\\nThe percentage: ' + str(per) + '%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total words: 2241041\n",
            "Infrequent words: 34240\n",
            "The percentage: 1.53%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBV8CTyk4KUJ"
      },
      "source": [
        "The `torchtext` package will help us keep track of our list of unique words, known\n",
        "as a **vocabulary**. A vocabulary also assigns a unique integer index to each word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRLFF8j14KUJ",
        "outputId": "68b409e2-a5b9-487f-b328-d44fcffed4d2"
      },
      "source": [
        "# Build the vocabulary based on the training data. The vocabulary\n",
        "# can have at most 9997 words (9995 words + the <bos> and <eos> token)\n",
        "text_field.build_vocab(train_data, max_size=9997)\n",
        " \n",
        "# This vocabulary object will be helpful for us\n",
        "vocab = text_field.vocab\n",
        "print(vocab.stoi[\"hello\"]) # for instances, we can convert from string to (unique) index\n",
        "print(vocab.itos[10])      # ... and from word index to string\n",
        " \n",
        "# The size of our vocabulary  \n",
        "vocab_size = len(text_field.vocab.stoi) \n",
        " \n",
        "# Here are the two tokens that torchtext adds for us:\n",
        "print(vocab.itos[0]) # <unk> represents an unknown word not in our vocabulary\n",
        "print(vocab.itos[1]) # <pad> will be used to pad short sequences for batching"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "on\n",
            "<unk>\n",
            "<pad>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnoMpzC14KUK"
      },
      "source": [
        "## Question 2. Text Autoencoder (40%)\n",
        "\n",
        "Building a text autoencoder is a little more complicated than an image autoencoder like we did in class. So\n",
        "we will need to thoroughly understand the model that we want to build before actually building it. Note that the best and fastest way to complete this assignment is to spend time upfront understanding the architecture. The explanations are quite dense, but it is important to understand the operation of this model. The rationale here is similar in nature to the `seq2seq` RNN model we discussed in class, only we are dealing with unsupervised learning here rather than machine translation. \n",
        "\n",
        "# Architecture description\n",
        "Here is a diagram showing our desired architecture:\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1UaXAYQvmFbBcuSBQb7ozjSliuVn7bP1Q'>\n",
        "\n",
        "There are two main components to the model: the **encoder** and the **decoder**.\n",
        "As always with neural networks, we'll first describe how to make\n",
        "**predictions** with of these components. Let's get started:\n",
        "\n",
        "The **encoder** will take a sequence of words (a headline) as *input*, and produce an\n",
        "embedding (a vector) that represents the entire headline. In the diagram above,\n",
        "the vector ${\\bf h}^{(7)}$ is the vector embedding containing information about \n",
        "the entire headline.  This portion is very similar\n",
        "to the sentiment analysis RNN that we discussed in lecture (but without the fully-connected\n",
        "layer that makes a prediction).\n",
        "\n",
        "The **decoder** will take an embedding (in the diagram, the vector ${\\bf h}^{(7)}$) as input,\n",
        "and uses a separate RNN to **generate a sequence of words**. To generate a sequence of words,\n",
        "the decoder needs to do the following:\n",
        "\n",
        "\n",
        "1.   Determine the previous word that was generated. This previous word will act as ${\\bf x}^{(t)}$\n",
        "   to our RNN, and will be used to update the hidden state ${\\bf m}^{(t)}$. Since each of our\n",
        "   sequences begin with the `<bos>` token, we'll set ${\\bf x}^{(1)}$ to be the `<bos>` token.\n",
        "2.   Compute the updates to the hidden state ${\\bf m}^{(t)}$ based on the previous hidden state\n",
        "   ${\\bf m}^{(t-1)}$ and ${\\bf x}^{(t)}$. Intuitively, this hidden state vector ${\\bf m}^{(t)}$\n",
        "   is a representation of *all the words we still need to generate*.\n",
        "3. We'll use a fully-connected layer to take a hidden state ${\\bf m}^{(t)}$, and determine\n",
        "   *what the next word should be*. This fully-connected layer solves a *classification problem*,\n",
        "   since we are trying to choose a word out of $K=$ `vocab_size` distinct words. As in a classification\n",
        "   problem, the fully-connected neural network will compute a *probability distribution* over\n",
        "   these `vocab_size` words. In the diagram, we are using ${\\bf z}^{(t)}$ to represent the logits,\n",
        "   or the pre-softmax activation values representing the probability distribution.\n",
        "4. We will need to *sample* an actual word from this probability distribution ${\\bf z}^{(t)}$.\n",
        "   We can do this in a number of ways, which we'll discuss in question 3. For now, you can \n",
        "   imagine your favourite way of picking a word given a distribution over words.\n",
        "5. This word we choose will become the next input ${\\bf x}^{(t+1)}$ to our RNN, which is used\n",
        "   to update our hidden state ${\\bf m}^{(t+1)}$, i.e., to determine what are the remaining\n",
        "   words to be generated.\n",
        "\n",
        "We can repeat this process until we see an `<eos>` token generated, or until the generated\n",
        "sequence becomes too long.\n",
        "\n",
        "# Training the architecture\n",
        "While our autoencoder produces a sequence, computing the loss by comparing the complete generated sequence to the ground truth (the encoder input) gives rise to multiple challanges. One is that the generated\n",
        "sequence might be longer or shorter than the actual sequence, meaning that there may\n",
        "be more/fewer ${\\bf z}^{(t)}$s than ground-truth words. Another more insidious issue\n",
        "is that the **gradients will become very high-variance and unstable**, because\n",
        "**early mistakes will easily throw the model off-track**. Early in training,\n",
        "our model is unlikely to produce the right answer in step $t=1$, so the gradients\n",
        "we obtain based on the other time steps will not be very useful.\n",
        "\n",
        "At this point, you might have some ideas about \"hacks\" we can use to make training\n",
        "work. Fortunately, there is one very well-established solution called\n",
        "**teacher forcing** which we can use for training:\n",
        "instead of *sampling* the next word based on ${\\bf z}^{(t)}$, we will forget sampling,\n",
        "and use the **ground truth** ${\\bf x}^{(t)}$ as the input in the next step.\n",
        "\n",
        "Here is a diagram showing how we can use **teacher forcing** to train our model:\n",
        " \n",
        "<img src='https://drive.google.com/uc?id=1NXlimWaCviDfP8DHlIzyQR1Ie7sxfPuY'>\n",
        "\n",
        "We will use the RNN generator to compute the logits\n",
        "${\\bf z}^{(1)},{\\bf z}^{(2)},  \\cdots {\\bf z}^{(T)}$. These distributions\n",
        "can be compared to the ground-truth words using the cross-entropy loss.\n",
        "The loss function for this model will be the sum of the losses across each $t \\in \\{1,\\ldots,T\\}$.\n",
        "\n",
        "We'll train the encoder and decoder model simultaneously. There are several components\n",
        "to our model that contain tunable weights:\n",
        "\n",
        "- The word embedding that maps a word to a vector representation.\n",
        "  In theory, we could use GloVe embeddings, as we did in class. In this assignment we will not do that, but learn the word embedding from data.\n",
        "  The word embedding component is represented with blue arrows in the diagram.\n",
        "- The encoder RNN (which will use GRUs) that computes the\n",
        "  embedding over the entire headline. The encoder RNN \n",
        "  is represented with black arrows in the diagram.\n",
        "- The decoder RNN (which will also use GRUs) that computes\n",
        "  hidden states, which are vectors representing what words are to be generated.\n",
        "  The decoder RNN is represented with gray arrows in the diagram.\n",
        "- The **projection MLP** (a fully-connected layer) that computes\n",
        "  a distribution over the next word to generate, given a decoder RNN hidden\n",
        "  state. The projection is represented with green arrows \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk9WDEC90ScA"
      },
      "source": [
        "\n",
        "## Part (a) -- 20%\n",
        "\n",
        "Complete the code for the AutoEncoder class below by:\n",
        "\n",
        "1. Filling in the missing numbers in the `__init__` method using\n",
        "   the parameters `vocab_size`, `emb_size`, and `hidden_size`.  \n",
        "2. Complete the `forward` method, which uses teacher forcing\n",
        "   and computes the logits ${\\bf z}^{(t)}$ of the reconstruction of\n",
        "   the sequence. \n",
        "\n",
        "You should first try to understand the `encode` and `decode` methods,\n",
        "which are written for you. The `encode` method bears much similarity to the\n",
        "RNN we wrote in class for sentiment analysis.  The `decode` method is\n",
        "a bit more challenging. You might want to scroll down to the\n",
        "`sample_sequence` function to see how this function will be called.\n",
        "\n",
        "You can (but don't have to) use the `encode` and `decode` method in\n",
        "your `forward` method. In either case, be careful of the input\n",
        "that you feed into ether `decode` or to `self.decoder_rnn`.\n",
        "Refer to the teacher-forcing diagram.\n",
        "**bold text** Notice that batch_first is set to True, understand how deal with it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wr5przESyHm"
      },
      "source": [
        "# model with for loop\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, hidden_size):\n",
        "        \"\"\"\n",
        "        A text autoencoder. The parameters \n",
        "            - vocab_size: number of unique words/tokens in the vocabulary\n",
        "            - emb_size: size of the word embeddings $x^{(t)}$\n",
        "            - hidden_size: size of the hidden states in both the\n",
        "                           encoder RNN ($h^{(t)}$) and the\n",
        "                           decoder RNN ($m^{(t)}$)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(num_embeddings = vocab_size, \n",
        "                                  embedding_dim = emb_size)  \n",
        "        self.encoder_rnn = nn.GRU(input_size = emb_size, \n",
        "                                  hidden_size = hidden_size, \n",
        "                                  batch_first = True)\n",
        "        self.decoder_rnn = nn.GRU(input_size = hidden_size, \n",
        "                                  hidden_size = hidden_size,\n",
        "                                  batch_first = True)\n",
        "        self.proj = nn.Linear(in_features = hidden_size, \n",
        "                              out_features = vocab_size) \n",
        " \n",
        "    def encode(self, inp):\n",
        "        \"\"\"\n",
        "        Computes the encoder output given a sequence of words.\n",
        "        \"\"\"\n",
        "        emb = self.embed(inp)\n",
        "        out, last_hidden = self.encoder_rnn(emb)\n",
        "        return last_hidden\n",
        " \n",
        "    def decode(self, inp, hidden=None):\n",
        "        \"\"\"\n",
        "        Computes the decoder output given a sequence of words, and\n",
        "        (optionally) an initial hidden state.\n",
        "        \"\"\"\n",
        "        emb = self.embed(inp)\n",
        "        out, last_hidden = self.decoder_rnn(emb, hidden)\n",
        "        out_seq = self.proj(out)\n",
        "        return out_seq, last_hidden\n",
        " \n",
        "    def forward(self, inp):\n",
        "        \"\"\"\n",
        "        Compute both the encoder and decoder forward pass\n",
        "        given an integer input sequence inp with shape [batch_size, seq_length],\n",
        "        with inp[a,b] representing the (index in our vocabulary of) the b-th word\n",
        "        of the a-th training example.\n",
        " \n",
        "        This function should return the logits $z^{(t)}$ in a tensor of shape\n",
        "        [batch_size, seq_length - 1, vocab_size], computed using *teaching forcing*.\n",
        " \n",
        "        The (seq_length - 1) part is not a typo. If you don't understand why\n",
        "        we need to subtract 1, refer to the teacher-forcing diagram above.\n",
        "        \"\"\"\n",
        " \n",
        "        \n",
        "        batch_size = inp.shape[0]\n",
        "        h = self.encode(inp.view(inp.shape[0], -1))\n",
        "        hidden = h\n",
        "        logits_z = []\n",
        " \n",
        "        for l in range(inp.shape[1] - 1):\n",
        "          z, m = self.decode(inp[:,l].view(batch_size, -1),hidden)\n",
        "          hidden = m\n",
        "          logits_z.append(z.view(batch_size,-1))\n",
        " \n",
        "        logits_z = torch.stack(logits_z)\n",
        "        logits_z = logits_z.permute(1, 0, 2)\n",
        " \n",
        "        return logits_z\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sU7-2JaVkTm",
        "outputId": "318faba2-3064-451a-bdcc-3d78d293f4de"
      },
      "source": [
        "#### TESTS\n",
        "# Initialize\n",
        "emb_size = 128\n",
        "hidden_size = 128\n",
        "model = AutoEncoder(vocab_size, emb_size, hidden_size)\n",
        " \n",
        "# forword with batch_size = 2 & len(headline) = 12 | input_shape = [2,12]\n",
        "headline1 = train_data[44].title\n",
        "headline2 = train_data[46].title\n",
        "input_seq1 = torch.Tensor([vocab.stoi[w] for w in headline1]).long().unsqueeze(0)\n",
        "input_seq2 = torch.Tensor([vocab.stoi[w] for w in headline2]).long().unsqueeze(0)\n",
        "input_seq = torch.cat((input_seq1,input_seq2),0)\n",
        "pred2 = model(input_seq)\n",
        "print('Test with batch_size = 2 \\ninput_shape = {} \\noutput_shape = {}\\n'.format(input_seq.shape,pred2.shape )) \n",
        "# print sentence \n",
        "sentence = []\n",
        "for word in range(pred2.shape[1]):\n",
        "  w_idx = torch.argmax(pred2[0,word,:])\n",
        "  w = vocab.itos[w_idx]\n",
        "  sentence.append(w)\n",
        "print(sentence)\n",
        "sentence = []\n",
        "for word in range(pred2.shape[1]):\n",
        "  w_idx = torch.argmax(pred2[1,word,:])\n",
        "  w = vocab.itos[w_idx]\n",
        "  sentence.append(w)\n",
        "print(sentence)\n",
        " \n",
        "print('\\n')\n",
        "# forword with batch_size = 1 & len(headline) = 9 | input_shape = [1,9]  \n",
        "headline = train_data[42].title\n",
        "input_seq = torch.Tensor([vocab.stoi[w] for w in headline]).long().unsqueeze(0)\n",
        "pred = model(input_seq)\n",
        "print('Test with batch_size = 1 \\ninput_shape = {} \\noutput_shape = {}\\n'.format(input_seq.shape,pred.shape )) \n",
        "sentence = []\n",
        " \n",
        "# print sentence  \n",
        "for word in range(pred.shape[1]):\n",
        "  w_idx = torch.argmax(pred[0,word,:])\n",
        "  w = vocab.itos[w_idx]\n",
        "  sentence.append(w)\n",
        "print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test with batch_size = 2 \n",
            "input_shape = torch.Size([2, 12]) \n",
            "output_shape = torch.Size([2, 11, 10000])\n",
            "\n",
            "['chapter', 'etf', 'acquire', 'acquire', 'gmt', 'studies', 'libya', 'draw', 'digs', 'bale', 'determine']\n",
            "['chapter', 'trespassing', 'bike', 'garden', 'murdering', 'murdering', 'transaction', 'his', 'buoys', '40,000', 'mid-atlantic']\n",
            "\n",
            "\n",
            "Test with batch_size = 1 \n",
            "input_shape = torch.Size([1, 9]) \n",
            "output_shape = torch.Size([1, 8, 10000])\n",
            "\n",
            "['expels', 'baker', 'wo', 'help', 'michigan', 'mail', 'neighborhood', 'meters']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uwpjvIw4KUQ"
      },
      "source": [
        "### Part (b) -- 10%\n",
        "\n",
        "To check that your model is set up correctly, we'll train our autoencoder\n",
        "neural network for at least 300 iterations to memorize this sequence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJmPNMnY4KUQ"
      },
      "source": [
        " \n",
        "headline = train_data[42].title\n",
        "input_seq = torch.Tensor([vocab.stoi[w] for w in headline]).long().unsqueeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj55uU-C4KUQ"
      },
      "source": [
        "We are looking for the way that you set up your loss function\n",
        "corresponding to the figure above.\n",
        "Be  careful of off-by-one errors here.\n",
        "\n",
        "\n",
        "Note that the Cross Entropy Loss expects a rank-2 tensor as its first\n",
        "argument (the output of the network), and a rank-1 tensor as its second argument (the true label). You will\n",
        "need to properly reshape your data to be able to compute the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGzfOt-inycq",
        "outputId": "783ca0cd-5749-426a-f1b2-20877a84f998"
      },
      "source": [
        "model = AutoEncoder(vocab_size, 128, 128)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        " \n",
        " \n",
        "for it in range(300):\n",
        "   \n",
        "  pred = model(input_seq)\n",
        " \n",
        "  pred_reshape = torch.reshape(pred, (-1,pred.shape[2]))\n",
        "  input_seq_reshape = input_seq[:,1:]\n",
        "  input_seq_reshape = torch.reshape(input_seq_reshape, (-1,))\n",
        "  \n",
        "  loss = criterion(pred_reshape,input_seq_reshape).sum()\n",
        "  loss.backward( retain_graph = True )                             # compute update for each parameter\n",
        "  optimizer.step()                            # make the updates for each parameter\n",
        "  optimizer.zero_grad()                       # a clean up step for PyTorch  \n",
        " \n",
        "  \n",
        "  if (it+1) % 50 == 0:\n",
        "    print(\"[Iter %d] Loss %f\" % (it+1, float(loss)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Iter 50] Loss 0.103191\n",
            "[Iter 100] Loss 0.026909\n",
            "[Iter 150] Loss 0.016125\n",
            "[Iter 200] Loss 0.010982\n",
            "[Iter 250] Loss 0.008075\n",
            "[Iter 300] Loss 0.006239\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aScYWbBx4KUQ"
      },
      "source": [
        "'''\n",
        "# amit\n",
        "model = AutoEncoder(vocab_size, 128, 128)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "for it in range(1):\n",
        " \n",
        "  # TODO - Not Complete\n",
        "   \n",
        "  pred = model(input_seq)\n",
        "\n",
        "  pred_reshape = torch.reshape(pred, (-1,pred.shape[2]))\n",
        "  input_seq_reshape = input_seq[:,1:]\n",
        "  input_seq_reshape = torch.reshape(input_seq_reshape, (-1,))\n",
        "  \n",
        "  loss = criterion(pred_new,input_seq_reshape).sum()\n",
        "  loss.backward( retain_graph = True )                             # compute update for each parameter\n",
        "  optimizer.step()                            # make the updates for each parameter\n",
        "  optimizer.zero_grad()                       # a clean up step for PyTorch  \n",
        "\n",
        "  \n",
        "  if (it+1) % 50 == 0:\n",
        "    print(\"[Iter %d] Loss %f\" % (it+1, float(loss)))\n",
        "    \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phxBSg217AgG"
      },
      "source": [
        "'''model = AutoEncoder(vocab_size, 128, 128)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        " \n",
        "# TODO - Not Complete\n",
        "optimizer.zero_grad()                       # a clean up step for PyTorch   \n",
        "pred = model(input_seq)\n",
        "\n",
        "pred_reshape = torch.reshape(pred, (-1,pred.shape[2]))\n",
        "input_seq_reshape = input_seq[:,1:]\n",
        "input_seq_reshape = torch.reshape(input_seq_reshape, (-1,))\n",
        "print(pred_new.shape)\n",
        "print(input_seq_reshape.shape)\n",
        "loss = criterion(pred_new,input_seq_reshape)\n",
        "print(loss)\n",
        "loss.backward()                             # compute update for each parameter\n",
        "optimizer.step()                            # make the updates for each parameter\n",
        "\n",
        "print(\"[Iter %d] Loss %f\" % (it+1, float(loss)))\n",
        " '''   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWKYC3z34KUR"
      },
      "source": [
        "### Part (c) -- 4%\n",
        "\n",
        "Once you are satisfied with your model, encode your input using\n",
        "the RNN encoder, and sample some sequences from the decoder. The \n",
        "sampling code is provided to you, and performs the computation\n",
        "from the first diagram (without teacher forcing).\n",
        "\n",
        "Note that we are sampling from a multi-nomial distribution described\n",
        "by the logits $z^{(t)}$. For example, if our distribution is [80%, 20%]\n",
        "over a vocabulary of two words, then we will choose the first word\n",
        "with 80% probability and the second word with 20% probability.\n",
        "\n",
        "Call `sample_sequence` at least 5 times, with the default temperature\n",
        "value. Make sure to include the generated sequences in your PDF\n",
        "report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vjVvmn14KUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b59c31da-e4ba-47c0-9847-e62f43628758"
      },
      "source": [
        "def sample_sequence(model, hidden, max_len=20, temperature=1):\n",
        "    \"\"\"\n",
        "    Return a sequence generated from the model's decoder\n",
        "        - model: an instance of the AutoEncoder model\n",
        "        - hidden: a hidden state (e.g. computed by the encoder)\n",
        "        - max_len: the maximum length of the generated sequence\n",
        "        - temperature: described in Part (d)\n",
        "    \"\"\"\n",
        "    # We'll store our generated sequence here\n",
        "    generated_sequence = []\n",
        "    # Set input to the <BOS> token\n",
        "    inp = torch.Tensor([text_field.vocab.stoi[\"<bos>\"]]).long()\n",
        "    for p in range(max_len):\n",
        "        # compute the output and next hidden unit\n",
        "        output, hidden = model.decode(inp.unsqueeze(0), hidden)\n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = int(torch.multinomial(output_dist, 1)[0])\n",
        "        # Add predicted word to string and use as next input\n",
        "        word = text_field.vocab.itos[top_i]\n",
        "        # Break early if we reach <eos>\n",
        "        if word == \"<eos>\":\n",
        "            break\n",
        "        generated_sequence.append(word)\n",
        "        inp = torch.Tensor([top_i]).long()\n",
        "    return generated_sequence\n",
        " \n",
        "h = model.encode(input_seq)\n",
        "gen_sq = sample_sequence(model=model, hidden=h, max_len=20, temperature=1)\n",
        "print(\"Input :\", end =\" \")\n",
        "print(headline[1:-1])\n",
        "print(\"Output:\", end =\" \")\n",
        "print(gen_sq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : ['zambian', 'president', 'swears', 'in', 'new', 'army', 'chief']\n",
            "Output: ['zambian', 'president', 'swears', 'in', 'new', 'army', 'chief']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T7gcPnn4KUR"
      },
      "source": [
        "### Part (d) -- 6%\n",
        "\n",
        "The multi-nomial distribution can be manipulated using the `temperature`\n",
        "setting. This setting can be used to make the distribution \"flatter\" (e.g.\n",
        "more likely to generate different words) or \"peakier\" (e.g. less likely\n",
        "to generate different words).\n",
        "\n",
        "Call `sample_sequence` at least 5 times each for at least 3 different\n",
        "temperature settings (e.g. 1.5, 2, and 5). Explain why we generally\n",
        "don't want the temperature setting to be too **large**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA_S0cmE4KUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a463a5f5-bd41-4623-82e6-67a604fa1a8f"
      },
      "source": [
        "h = model.encode(input_seq)\n",
        "print(\"Input: \", end =\" \")\n",
        "print(headline[1:-1])\n",
        "tm_set = (1.5, 2, 5)\n",
        "for tm in tm_set:\n",
        "  print(\"Temperature = \"+str(tm)+\":\")\n",
        "  for i in range(5):\n",
        "    gen_sq = sample_sequence(model=model, hidden=h, max_len=20, temperature=tm)\n",
        "    print(str(i+1) +\") \", end =\" \")\n",
        "    print(gen_sq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:  ['zambian', 'president', 'swears', 'in', 'new', 'army', 'chief']\n",
            "Temperature = 1.5:\n",
            "1)  ['zambian', 'president', 'oregon', 'rolls-royce', '-cnbc', 'exercises', 'var', 'virgin', 'in', 'new', 'leaks', 'zambian', 'reservations', 'new', 'army', 'chief', 'medal', 'roles', 'regrettable', 'dampen']\n",
            "2)  ['zambian', 'swift', 'army', 'chief', 'appears', 'denel', 'delivers', 'ryan', 'hope', 'kenin']\n",
            "3)  ['pernod', 'bndes', 'swears', 'in', '_num_-investor', 'mcilroy', 'eke', 'swap', 'swears', 'in', 'pro-government', 'phase', 'taylor', '_num_-motor', 'safe-havens', 'improved', 'information', 'reporters', 'mind', 'balkan']\n",
            "4)  ['zambian', 'president', 'individuals', 'in', 'new', 'tip', 'dozen', 'filing']\n",
            "5)  ['zambian', 'invests', 'president', 'swears', 'h1', 'mistake', 'insecurity', 'firms', 'reais']\n",
            "Temperature = 2:\n",
            "1)  ['residents', 'forbes', 'dialogue', 'mourinho', 'departs', 'texts', 'concessions', 'forces', 'faster', 'legacy', 'played', 'swears', 'in', 'aircraft', 'bombshell', 'army', 'pens', 'exposed', 'prisoners', 'markets-dollar']\n",
            "2)  ['basin', 'agent', 'alleging', 'cup', 'erases', 'markets-brazilian', 'paulo', 'fix', 'title', 'joy', 'loyal', 'president', 'jan.', 'extremist', 'new', 'hospital', 'burst', 'dissident', 'commodities', '-traders']\n",
            "3)  ['portfolio', 'tepid', 'onshore', 'military', 'mid-atlantic', 'hacking', 'care', 'refile-us']\n",
            "4)  ['unaware', 'licenses', 'tumble', 'chief', 'rutte', 'congolese', 'virus-hit', 'denel', 'galactic', 'national', 'chase', 'dairy', '100,000', 'chief', 'tribute', 'reagan', 'demise', 'non-binding', 'downed', 'priority']\n",
            "5)  ['medvedev', 'debt-c', 'milestone', 'organic', 'bladder', '_num_-target', 'snowstorm', 'animal', 'chief', 'dhabi', 'stand-off', 'unchanged', 'skills', 'dock', 'vertex', 'name', 'repatriation', 'dior', 'freeland', 'maiden']\n",
            "Temperature = 5:\n",
            "1)  ['demanding', 'origins', '-wsj', 'u.s.-mexico-canada', 'contractors', 'botched', '-cnbc', 'leaves', 'her', 'buffett', 'ski', 'safely', 'illinois', 'electoral', 'david', 'difficult', 'driverless', 'franc', 'urge', 'middle']\n",
            "2)  ['african', 'quotas', '_num_-shares', 'wada', 'leadership', 'revamped', 'mcdonald', 'damascus', 'campaigners', 'detects', 'eke', 'wildfire', 'hung', 'novel', 'passports', 'nikkei', 'request', 'problems', 'reinstates', 'marred']\n",
            "3)  ['bearish', 'wednesday', 'terminals', 'capri', 'votes', 'america', 'stabbing', 'kidnapped', 'relation', 'class', 'investigating', 'inject', 'searching', 'elon', 'forex-euro', 'outage', 'cars', 'disappoint', 'received', 'biggest']\n",
            "4)  ['network', 'parents', 'fox', 'spokesman', 'blackrock', 'astrazeneca', 'based', 'no-deal', 'additions', 'reaches', 'targeted', 'mount', 'lane', 'chuquicamata', 'mitiga', 'collective', 'moments', 'talent', 'solution', 'picture']\n",
            "5)  ['offensive', 'green', 'dwindles', 'cabin', 'capped', 'nations', 'francis', 'promising', 'chosen', 'albemarle', 'stamps', 'travellers', 'practice', 'shortly', 'florida', 'santiago', 'task', 'test', 'baker', 'second-quarter']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZkJ_b_0lA37"
      },
      "source": [
        "**Answer**:\n",
        "\n",
        "We may naturally prefer not setting the temperature to be excessively large, mostly because, as we increase the temperature - the variance increases, thus less words from the original input headline appears in the outcome. \n",
        "\n",
        "It also been observed that the possible sentences becoming longer with less correlation between the words.\n",
        "\n",
        "Both of those reasons results with illogical outputs sentences.\n",
        "\n",
        "For the given example:\n",
        "* 1.5 for the temperature: contains the largest amount of the words from the input headline, from all of the given temperatures settings.\n",
        "* 2 for the temperature: there is a decline in the appearing of the original words.\n",
        "* 5 for the temperature: there are no words from the original inputted headline at all. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK4ptUb54KUS"
      },
      "source": [
        "## Question 3. Data augmentation (20%)\n",
        "\n",
        "It turns out that getting good results from a text auto-encoder is very difficult,\n",
        "and that it is very easy for our model to **overfit**. We have discussed several methods\n",
        "that we can use to prevent overfitting, and we'll introduce one more today:\n",
        "**data augmentation**.\n",
        "\n",
        "The idea behind data augmentation is to artificially increase the number of training\n",
        "examples by \"adding noise\" to the image. For example, during AlexNet training,\n",
        "the authors randomly cropped $224\\times 224$\n",
        "regions of a $256 \\times 256$ pixel image to increase the amount of training data.\n",
        "The authors also flipped the image left/right.\n",
        "Machine learning practitioners can also add Gaussian noise to the image.\n",
        "\n",
        "When we use data augmentation to train an *autoencoder*, we typically to only add\n",
        "the noise to the input, and expect the reconstruction to be *noise free*.\n",
        "This makes the task of the autoencoder even more difficult. An autoencoder trained\n",
        "with noisy inputs is called a **denoising auto-encoder**. For simplicity, we will\n",
        "*not* build a denoising autoencoder today.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yieWyWqS4KUS"
      },
      "source": [
        "### Part (a) -- 5%\n",
        "\n",
        "We will add noise to our headlines using a few different techniques:\n",
        "\n",
        "1. Shuffle the words in the headline, taking care that words don't end up too far from where they were initially\n",
        "2. Drop (remove) some words \n",
        "3. Replace some words with a blank word (a `<pad>` token)\n",
        "4. Replace some words with a random word \n",
        "\n",
        "The code for adding these types of noise is provided for you:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGb3uL5U4KUS"
      },
      "source": [
        "def tokenize_and_randomize(headline,\n",
        "                           drop_prob=0.1,  # probability of dropping a word\n",
        "                           blank_prob=0.1, # probability of \"blanking\" out a word\n",
        "                           sub_prob=0.1,   # probability of substituting a word with a random one\n",
        "                           shuffle_dist=3): # maximum distance to shuffle a word\n",
        "    \"\"\"\n",
        "    Add 'noise' to a headline by slightly shuffling the word order,\n",
        "    dropping some words, blanking out some words (replacing with the <pad> token)\n",
        "    and substituting some words with random ones.\n",
        "    \"\"\"\n",
        "    headline = [vocab.stoi[w] for w in headline]\n",
        "    n = len(headline)\n",
        "    # shuffle\n",
        "    headline = [headline[i] for i in get_shuffle_index(n, shuffle_dist)]\n",
        "\n",
        "    new_headline = [vocab.stoi['<bos>']]\n",
        "    for w in headline:\n",
        "        if random.random() < drop_prob:\n",
        "            # drop the word\n",
        "            pass\n",
        "        elif random.random() < blank_prob:\n",
        "            # replace with blank word\n",
        "            new_headline.append(vocab.stoi[\"<pad>\"])\n",
        "        elif random.random() < sub_prob:\n",
        "            # substitute word with another word\n",
        "            new_headline.append(random.randint(0, vocab_size - 1))\n",
        "        else:\n",
        "            # keep the original word\n",
        "            new_headline.append(w)\n",
        "    new_headline.append(vocab.stoi['<eos>'])\n",
        "    return new_headline\n",
        "\n",
        "def get_shuffle_index(n, max_shuffle_distance):\n",
        "    \"\"\" This is a helper function used to shuffle a headline with n words,\n",
        "    where each word is moved at most max_shuffle_distance. The function does\n",
        "    the following: \n",
        "       1. start with the *unshuffled* index of each word, which\n",
        "          is just the values [0, 1, 2, ..., n]\n",
        "       2. perturb these \"index\" values by a random floating-point value between\n",
        "          [0, max_shuffle_distance]\n",
        "       3. use the sorted position of these values as our new index\n",
        "    \"\"\"\n",
        "    index = np.arange(n)\n",
        "    perturbed_index = index + np.random.rand(n) * 3\n",
        "    new_index = sorted(enumerate(perturbed_index), key=lambda x: x[1])\n",
        "    return [index for (index, pert) in new_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8t6eZgy4KUT"
      },
      "source": [
        "Call the function `tokenize_and_randomize` 5 times on a headline of your\n",
        "choice. Make sure to include both your original headline, and the five new\n",
        "headlines in your report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CktsYMIE4KUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bcc75e1-be19-47f1-ae28-2e963e4761d8"
      },
      "source": [
        "hdln = train_data[1193].title \n",
        "hdln = hdln[1:-1]\n",
        "print(\"Original headline:\", end =\" \")\n",
        "print(hdln)\n",
        "\n",
        "print(\"Augmentet headlines:\")\n",
        "for i in range(5):\n",
        "  print(str(i+1) +\") \", end =\" \")\n",
        "\n",
        "  augm = tokenize_and_randomize(headline=hdln)\n",
        "  sentence = []\n",
        "  for w_idx in augm:\n",
        "    wrd = vocab.itos[w_idx]\n",
        "    sentence.append(wrd)\n",
        "  print(sentence[1:-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original headline: ['syrian', 'opposition', 'says', 'surprised', 'by', 'countries', 'reconciling', 'with', 'assad']\n",
            "Augmentet headlines:\n",
            "1)  ['syrian', 'says', 'reports', 'countries', 'with', '<unk>', 'charts']\n",
            "2)  ['<pad>', 'opposition', 'surprised', 'by', 'says', 'countries', 'with', '<unk>', 'assad']\n",
            "3)  ['syrian', 'opposition', 'says', 'countries', '<unk>', 'assad', 'with']\n",
            "4)  ['syrian', 'opposition', '<pad>', 'by', 'assad', 'with']\n",
            "5)  ['syrian', 'opposition', 'surprised', 'says', 'by', 'countries', 'with', '<unk>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHKlJr_m4KUT"
      },
      "source": [
        "### Part (b) -- 8%\n",
        "\n",
        "The training code that we use to train the model is mostly provided for you. \n",
        "The only part we left blank are the parts from Q2(b). Complete the code,\n",
        "and train a new AutoEncoder model for 1 epoch. You can train your model\n",
        "for longer if you want, but training tend to take a long time,\n",
        "so we're only checking to see that your training loss is trending down.\n",
        "\n",
        "If you are using Google Colab, you can use a GPU for this portion.\n",
        "Go to \"Runtime\" => \"Change Runtime Type\"  and set \"Hardware acceleration\" to GPU.\n",
        "Your Colab session will restart.\n",
        "You can move your model to the GPU by typing `model.cuda()`, and move\n",
        "other tensors to GPU (e.g. `xs = xs.cuda()`). To move a model back to CPU,\n",
        "type `model.cpu`. To move a tensor back, use `xs = xs.cpu()`. For training,\n",
        "your model and inputs need to be on the *same device*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IXnUFUJ4KUT"
      },
      "source": [
        "'''\n",
        "def train_autoencoder(model, batch_size=64, learning_rate=0.001, num_epochs=10):\n",
        "    model.train()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for ep in range(num_epochs):\n",
        "        # We will perform data augmentation by re-reading the input each time\n",
        "        field = data.Field(sequential=True,\n",
        "                                      tokenize=tokenize_and_randomize, # <-- data augmentation\n",
        "                                      include_lengths=True,\n",
        "                                      batch_first=True,\n",
        "                                      use_vocab=False, # <-- the tokenization function replaces this\n",
        "                                      pad_token=vocab.stoi['<pad>'])\n",
        "        dataset = data.TabularDataset(train_path, \"tsv\", [('title', field)])\n",
        "\n",
        "        # This BucketIterator will handle padding of sequences that are not of the same length\n",
        "        train_iter = data.BucketIterator(dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          sort_key=lambda x: len(x.title), # to minimize padding\n",
        "                                          repeat=False)\n",
        "        for it, ((xs, lengths), _) in enumerate(train_iter):\n",
        "            pred = model(xs)\n",
        "\n",
        "            pred_reshape = torch.reshape(pred, (-1,pred.shape[2]))\n",
        "            input_seq_reshape = xs[:,1:]\n",
        "            input_seq_reshape = torch.reshape(input_seq_reshape, (-1,))\n",
        "\n",
        "            loss = criterion(pred_reshape,input_seq_reshape).sum()\n",
        "            loss.backward( retain_graph = True ) # compute update for each parameter\n",
        "            optimizer.step() # make the updates for each parameter\n",
        "            optimizer.zero_grad()   \n",
        "\n",
        "            if (it+1) % 100 == 0:\n",
        "                print(\"[Iter %d] Loss %f\" % (it+1, float(loss)))\n",
        "\n",
        "            # Optional: Compute and track validation loss\n",
        "            #val_loss = 0\n",
        "            #val_n = 0\n",
        "            #for it, ((xs, lengths), _) in enumerate(valid_iter):\n",
        "            #    zs = model(xs)\n",
        "            #    loss = criterion(zs,xs).sum()\n",
        "            #    val_loss += float(loss)\n",
        "\n",
        "'''\n",
        "# Include your training curve or output to show that your training loss is trending down"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dILV9Gqv-OoI",
        "outputId": "b63d0411-cfcb-43e2-975f-a27728b754b0"
      },
      "source": [
        "'''model = AutoEncoder(vocab_size, 128, 128)\n",
        "train_autoencoder(model, batch_size=64, learning_rate=0.001, num_epochs=1)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Iter 100] Loss 0.659577\n",
            "[Iter 200] Loss 0.503200\n",
            "[Iter 300] Loss 0.488491\n",
            "[Iter 400] Loss 0.360810\n",
            "[Iter 500] Loss 0.327694\n",
            "[Iter 600] Loss 0.202819\n",
            "[Iter 700] Loss 0.131260\n",
            "[Iter 800] Loss 0.081393\n",
            "[Iter 900] Loss 0.033916\n",
            "[Iter 1000] Loss 0.020573\n",
            "[Iter 1100] Loss 0.012692\n",
            "[Iter 1200] Loss 0.008000\n",
            "[Iter 1300] Loss 0.006637\n",
            "[Iter 1400] Loss 0.006269\n",
            "[Iter 1500] Loss 0.004268\n",
            "[Iter 1600] Loss 0.003764\n",
            "[Iter 1700] Loss 0.002854\n",
            "[Iter 1800] Loss 0.002688\n",
            "[Iter 1900] Loss 0.002601\n",
            "[Iter 2000] Loss 0.002093\n",
            "[Iter 2100] Loss 0.001855\n",
            "[Iter 2200] Loss 0.001766\n",
            "[Iter 2300] Loss 0.001477\n",
            "[Iter 2400] Loss 0.001333\n",
            "[Iter 2500] Loss 0.001282\n",
            "[Iter 2600] Loss 0.001083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bFrk5UvHwW2e",
        "outputId": "d72f6b07-60cc-421b-cc7d-61e261b5ab89"
      },
      "source": [
        "def train_autoencoder(model, batch_size=64, learning_rate=0.001, num_epochs=10):\n",
        "    model.cuda() \n",
        "    #model.cpu\n",
        "    model.train()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    max_iter = 0\n",
        "    losses, iters = [], []\n",
        "    #val_losses =[]\n",
        "    for ep in range(num_epochs):\n",
        "        print(\"Epoch \" + str(ep))\n",
        "        # We will perform data augmentation by re-reading the input each time\n",
        "        field = data.Field(sequential=True,\n",
        "                                      tokenize=tokenize_and_randomize, # <-- data augmentation\n",
        "                                      include_lengths=True,\n",
        "                                      batch_first=True,\n",
        "                                      use_vocab=False, # <-- the tokenization function replaces this\n",
        "                                      pad_token=vocab.stoi['<pad>'])\n",
        "        dataset = data.TabularDataset(train_path, \"tsv\", [('title', field)])\n",
        "\n",
        "\n",
        "        # This BucketIterator will handle padding of sequences that are not of the same length\n",
        "        train_iter = data.BucketIterator(dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          sort_key=lambda x: len(x.title), # to minimize padding\n",
        "                                          repeat=False)\n",
        "        for it, ((xs, lengths), _) in enumerate(train_iter):\n",
        "            xs = xs.cuda()\n",
        "            pred = model(xs)\n",
        "            \n",
        "            pred_reshape = torch.reshape(pred, (-1,pred.shape[2]))\n",
        "            input_seq_reshape = xs[:,1:]\n",
        "            input_seq_reshape = torch.reshape(input_seq_reshape, (-1,))\n",
        "\n",
        "            loss = criterion(pred_reshape,input_seq_reshape).sum()\n",
        "            loss.backward( retain_graph = True ) # compute update for each parameter\n",
        "            optimizer.step() # make the updates for each parameter\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            \n",
        "\n",
        "            # Optional: Compute and track validation loss\n",
        "            #val_loss = 0\n",
        "            #val_n = 0\n",
        "            #for it, ((xs, lengths), _) in enumerate(valid_iter):\n",
        "            #    zs = model(xs)\n",
        "            #    loss = criterion(zs,xs).sum()\n",
        "            #    val_loss += float(loss)\n",
        "            \n",
        "            if (it+1) % 100 == 0: \n",
        "                print(\"[Iter %d] Loss %f\" % (it+1, float(loss)))\n",
        "                it_n = ep * max_iter + it+1\n",
        "                iters.append(it_n)\n",
        "                losses.append(float(loss))\n",
        "                #val_losses.append(float(val_loss))\n",
        "        max_iter = it+1\n",
        "    \n",
        "    model.cpu\n",
        "\n",
        "    # Prepare data    \n",
        "    iters = np.array(iters)\n",
        "    losses = np.array(losses)\n",
        "    #val_losses = np.array(val_losses)\n",
        "\n",
        "    # Plot the training curve\n",
        "    plt.title(\"Learning Curve: Loss per Iteration\")\n",
        "    plt.plot(iters, losses, label=\"Train\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Include your training curve or output to show that your training loss is trending down\n",
        "model = AutoEncoder(vocab_size, 128, 128)\n",
        "train_autoencoder(model, batch_size=64, learning_rate=0.001, num_epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "[Iter 100] Loss 2.527606\n",
            "[Iter 200] Loss 2.468636\n",
            "[Iter 300] Loss 2.441853\n",
            "[Iter 400] Loss 2.511398\n",
            "[Iter 500] Loss 2.465988\n",
            "[Iter 600] Loss 2.511451\n",
            "[Iter 700] Loss 2.617448\n",
            "[Iter 800] Loss 2.310770\n",
            "[Iter 900] Loss 2.434045\n",
            "[Iter 1000] Loss 2.363087\n",
            "[Iter 1100] Loss 2.299358\n",
            "[Iter 1200] Loss 2.401775\n",
            "[Iter 1300] Loss 2.433352\n",
            "[Iter 1400] Loss 2.283909\n",
            "[Iter 1500] Loss 2.543908\n",
            "[Iter 1600] Loss 2.016899\n",
            "[Iter 1700] Loss 2.265496\n",
            "[Iter 1800] Loss 2.663023\n",
            "[Iter 1900] Loss 2.152513\n",
            "[Iter 2000] Loss 2.101025\n",
            "[Iter 2100] Loss 2.193232\n",
            "[Iter 2200] Loss 2.183590\n",
            "[Iter 2300] Loss 2.162126\n",
            "[Iter 2400] Loss 2.101472\n",
            "[Iter 2500] Loss 2.200275\n",
            "[Iter 2600] Loss 2.266797\n",
            "Epoch 1\n",
            "[Iter 100] Loss 2.211490\n",
            "[Iter 200] Loss 2.254715\n",
            "[Iter 300] Loss 2.053586\n",
            "[Iter 400] Loss 1.949518\n",
            "[Iter 500] Loss 1.918189\n",
            "[Iter 600] Loss 2.095116\n",
            "[Iter 700] Loss 2.100752\n",
            "[Iter 800] Loss 2.212242\n",
            "[Iter 900] Loss 2.096970\n",
            "[Iter 1000] Loss 2.038052\n",
            "[Iter 1100] Loss 2.067373\n",
            "[Iter 1200] Loss 2.179970\n",
            "[Iter 1300] Loss 1.577055\n",
            "[Iter 1400] Loss 1.982650\n",
            "[Iter 1500] Loss 2.109055\n",
            "[Iter 1600] Loss 2.071373\n",
            "[Iter 1700] Loss 2.133854\n",
            "[Iter 1800] Loss 2.140198\n",
            "[Iter 1900] Loss 1.707946\n",
            "[Iter 2000] Loss 2.257347\n",
            "[Iter 2100] Loss 1.935053\n",
            "[Iter 2200] Loss 2.041567\n",
            "[Iter 2300] Loss 2.093872\n",
            "[Iter 2400] Loss 1.989159\n",
            "[Iter 2500] Loss 1.960676\n",
            "[Iter 2600] Loss 1.875070\n",
            "Epoch 2\n",
            "[Iter 100] Loss 2.031921\n",
            "[Iter 200] Loss 2.022674\n",
            "[Iter 300] Loss 2.009073\n",
            "[Iter 400] Loss 1.895192\n",
            "[Iter 500] Loss 1.913489\n",
            "[Iter 600] Loss 1.908301\n",
            "[Iter 700] Loss 2.162522\n",
            "[Iter 800] Loss 1.750598\n",
            "[Iter 900] Loss 1.917496\n",
            "[Iter 1000] Loss 1.625327\n",
            "[Iter 1100] Loss 1.947224\n",
            "[Iter 1200] Loss 2.018059\n",
            "[Iter 1300] Loss 1.675233\n",
            "[Iter 1400] Loss 1.833837\n",
            "[Iter 1500] Loss 1.830830\n",
            "[Iter 1600] Loss 1.766907\n",
            "[Iter 1700] Loss 1.600175\n",
            "[Iter 1800] Loss 1.992490\n",
            "[Iter 1900] Loss 1.807895\n",
            "[Iter 2000] Loss 1.816756\n",
            "[Iter 2100] Loss 1.803151\n",
            "[Iter 2200] Loss 1.990305\n",
            "[Iter 2300] Loss 1.879312\n",
            "[Iter 2400] Loss 1.813530\n",
            "[Iter 2500] Loss 2.016708\n",
            "[Iter 2600] Loss 2.078932\n",
            "Epoch 3\n",
            "[Iter 100] Loss 1.880595\n",
            "[Iter 200] Loss 1.802324\n",
            "[Iter 300] Loss 1.911558\n",
            "[Iter 400] Loss 1.790117\n",
            "[Iter 500] Loss 2.045859\n",
            "[Iter 600] Loss 1.950211\n",
            "[Iter 700] Loss 1.973041\n",
            "[Iter 800] Loss 1.679859\n",
            "[Iter 900] Loss 1.735942\n",
            "[Iter 1000] Loss 1.942980\n",
            "[Iter 1100] Loss 1.832662\n",
            "[Iter 1200] Loss 1.954228\n",
            "[Iter 1300] Loss 1.858953\n",
            "[Iter 1400] Loss 1.964244\n",
            "[Iter 1500] Loss 1.722031\n",
            "[Iter 1600] Loss 2.027869\n",
            "[Iter 1700] Loss 1.903651\n",
            "[Iter 1800] Loss 1.946490\n",
            "[Iter 1900] Loss 1.776857\n",
            "[Iter 2000] Loss 1.827438\n",
            "[Iter 2100] Loss 1.908995\n",
            "[Iter 2200] Loss 1.832108\n",
            "[Iter 2300] Loss 1.963661\n",
            "[Iter 2400] Loss 1.954535\n",
            "[Iter 2500] Loss 1.886177\n",
            "[Iter 2600] Loss 1.904860\n",
            "Epoch 4\n",
            "[Iter 100] Loss 1.764878\n",
            "[Iter 200] Loss 1.933945\n",
            "[Iter 300] Loss 1.867038\n",
            "[Iter 400] Loss 1.734959\n",
            "[Iter 500] Loss 1.692304\n",
            "[Iter 600] Loss 1.627347\n",
            "[Iter 700] Loss 1.856024\n",
            "[Iter 800] Loss 1.625979\n",
            "[Iter 900] Loss 1.983394\n",
            "[Iter 1000] Loss 1.898938\n",
            "[Iter 1100] Loss 1.855685\n",
            "[Iter 1200] Loss 1.684664\n",
            "[Iter 1300] Loss 1.863338\n",
            "[Iter 1400] Loss 1.947116\n",
            "[Iter 1500] Loss 1.771772\n",
            "[Iter 1600] Loss 1.840814\n",
            "[Iter 1700] Loss 1.844813\n",
            "[Iter 1800] Loss 1.893789\n",
            "[Iter 1900] Loss 1.747263\n",
            "[Iter 2000] Loss 1.905916\n",
            "[Iter 2100] Loss 1.879239\n",
            "[Iter 2200] Loss 1.713523\n",
            "[Iter 2300] Loss 1.754627\n",
            "[Iter 2400] Loss 1.784333\n",
            "[Iter 2500] Loss 1.929189\n",
            "[Iter 2600] Loss 1.812662\n",
            "Epoch 5\n",
            "[Iter 100] Loss 1.717537\n",
            "[Iter 200] Loss 1.822006\n",
            "[Iter 300] Loss 1.575634\n",
            "[Iter 400] Loss 1.831697\n",
            "[Iter 500] Loss 1.661880\n",
            "[Iter 600] Loss 1.921058\n",
            "[Iter 700] Loss 1.770223\n",
            "[Iter 800] Loss 1.841710\n",
            "[Iter 900] Loss 1.714883\n",
            "[Iter 1000] Loss 1.723453\n",
            "[Iter 1100] Loss 1.840916\n",
            "[Iter 1200] Loss 1.803573\n",
            "[Iter 1300] Loss 1.592245\n",
            "[Iter 1400] Loss 1.917934\n",
            "[Iter 1500] Loss 1.505492\n",
            "[Iter 1600] Loss 1.832587\n",
            "[Iter 1700] Loss 1.682159\n",
            "[Iter 1800] Loss 1.823857\n",
            "[Iter 1900] Loss 1.851969\n",
            "[Iter 2000] Loss 1.578841\n",
            "[Iter 2100] Loss 1.813404\n",
            "[Iter 2200] Loss 1.531699\n",
            "[Iter 2300] Loss 1.909394\n",
            "[Iter 2400] Loss 1.739092\n",
            "[Iter 2500] Loss 1.568434\n",
            "[Iter 2600] Loss 1.651982\n",
            "Epoch 6\n",
            "[Iter 100] Loss 1.777838\n",
            "[Iter 200] Loss 1.562487\n",
            "[Iter 300] Loss 1.339912\n",
            "[Iter 400] Loss 1.689798\n",
            "[Iter 500] Loss 1.692287\n",
            "[Iter 600] Loss 1.801104\n",
            "[Iter 700] Loss 1.783838\n",
            "[Iter 800] Loss 1.721084\n",
            "[Iter 900] Loss 1.603738\n",
            "[Iter 1000] Loss 1.616114\n",
            "[Iter 1100] Loss 1.544720\n",
            "[Iter 1200] Loss 1.787267\n",
            "[Iter 1300] Loss 1.525311\n",
            "[Iter 1400] Loss 1.707694\n",
            "[Iter 1500] Loss 1.739637\n",
            "[Iter 1600] Loss 1.794711\n",
            "[Iter 1700] Loss 1.758454\n",
            "[Iter 1800] Loss 1.646814\n",
            "[Iter 1900] Loss 1.682451\n",
            "[Iter 2000] Loss 1.652869\n",
            "[Iter 2100] Loss 1.642928\n",
            "[Iter 2200] Loss 1.691167\n",
            "[Iter 2300] Loss 1.560334\n",
            "[Iter 2400] Loss 1.664535\n",
            "[Iter 2500] Loss 1.663094\n",
            "[Iter 2600] Loss 1.664362\n",
            "Epoch 7\n",
            "[Iter 100] Loss 1.686535\n",
            "[Iter 200] Loss 1.595328\n",
            "[Iter 300] Loss 1.880706\n",
            "[Iter 400] Loss 1.683560\n",
            "[Iter 500] Loss 1.762098\n",
            "[Iter 600] Loss 1.604855\n",
            "[Iter 700] Loss 1.736224\n",
            "[Iter 800] Loss 1.669702\n",
            "[Iter 900] Loss 1.567178\n",
            "[Iter 1000] Loss 1.503718\n",
            "[Iter 1100] Loss 1.700329\n",
            "[Iter 1200] Loss 1.569889\n",
            "[Iter 1300] Loss 1.619601\n",
            "[Iter 1400] Loss 1.571930\n",
            "[Iter 1500] Loss 1.716551\n",
            "[Iter 1600] Loss 1.619649\n",
            "[Iter 1700] Loss 1.883420\n",
            "[Iter 1800] Loss 1.769244\n",
            "[Iter 1900] Loss 1.642026\n",
            "[Iter 2000] Loss 1.674347\n",
            "[Iter 2100] Loss 1.494724\n",
            "[Iter 2200] Loss 1.519211\n",
            "[Iter 2300] Loss 1.612404\n",
            "[Iter 2400] Loss 1.614451\n",
            "[Iter 2500] Loss 1.445382\n",
            "[Iter 2600] Loss 1.658546\n",
            "Epoch 8\n",
            "[Iter 100] Loss 1.745218\n",
            "[Iter 200] Loss 1.559897\n",
            "[Iter 300] Loss 1.660750\n",
            "[Iter 400] Loss 1.537770\n",
            "[Iter 500] Loss 1.643376\n",
            "[Iter 600] Loss 1.591923\n",
            "[Iter 700] Loss 1.688274\n",
            "[Iter 800] Loss 1.625467\n",
            "[Iter 900] Loss 1.630294\n",
            "[Iter 1000] Loss 1.654242\n",
            "[Iter 1100] Loss 1.496433\n",
            "[Iter 1200] Loss 1.632552\n",
            "[Iter 1300] Loss 1.564475\n",
            "[Iter 1400] Loss 1.525404\n",
            "[Iter 1500] Loss 1.582529\n",
            "[Iter 1600] Loss 1.625668\n",
            "[Iter 1700] Loss 1.849087\n",
            "[Iter 1800] Loss 1.628953\n",
            "[Iter 1900] Loss 1.739387\n",
            "[Iter 2000] Loss 1.787530\n",
            "[Iter 2100] Loss 1.623564\n",
            "[Iter 2200] Loss 1.636854\n",
            "[Iter 2300] Loss 1.593303\n",
            "[Iter 2400] Loss 1.571515\n",
            "[Iter 2500] Loss 1.545393\n",
            "[Iter 2600] Loss 1.703493\n",
            "Epoch 9\n",
            "[Iter 100] Loss 1.536851\n",
            "[Iter 200] Loss 1.543490\n",
            "[Iter 300] Loss 1.524628\n",
            "[Iter 400] Loss 1.646659\n",
            "[Iter 500] Loss 1.661937\n",
            "[Iter 600] Loss 1.752152\n",
            "[Iter 700] Loss 1.569047\n",
            "[Iter 800] Loss 1.616985\n",
            "[Iter 900] Loss 1.694529\n",
            "[Iter 1000] Loss 1.649150\n",
            "[Iter 1100] Loss 1.690007\n",
            "[Iter 1200] Loss 1.642454\n",
            "[Iter 1300] Loss 1.614590\n",
            "[Iter 1400] Loss 1.525986\n",
            "[Iter 1500] Loss 1.656843\n",
            "[Iter 1600] Loss 1.454379\n",
            "[Iter 1700] Loss 1.598843\n",
            "[Iter 1800] Loss 1.484175\n",
            "[Iter 1900] Loss 1.609222\n",
            "[Iter 2000] Loss 1.629212\n",
            "[Iter 2100] Loss 1.584957\n",
            "[Iter 2200] Loss 1.584664\n",
            "[Iter 2300] Loss 1.483566\n",
            "[Iter 2400] Loss 1.580545\n",
            "[Iter 2500] Loss 1.707641\n",
            "[Iter 2600] Loss 1.829552\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9d5gkV3n2fT8VOk6enQ2zUdqg1SonQEJCAgESQkaYYDBYBAOyDK8Bm2j8ok+8gMEGgxCyABGEMSBMEEYkRRTQKq7yZm1OM7OTu6dzdZ3vj6pz6lR1dU9P6N0J53dde22Hit095z5PPMQYg0KhUCjmL9rxvgCFQqFQHF+UECgUCsU8RwmBQqFQzHOUECgUCsU8RwmBQqFQzHOUECgUCsU8RwmBYtIQ0UVEtON4X4di7kJE7ySiu4/3dcx1lBDMUohoHxG9+nheA2Psz4yxkxp1fCK6jIgeIqI0EfUT0YNE9IZGnW+yENF7iOjh430d0w0RXUJEh6TnDxDR+xt4vlVExIjI4K8xxn7CGHtto86pcFBCoKgKEenH8dxvAfALAD8CsAzAIgDXAfiLSRyLiEj91msgD74NPMdx+z0paqP+OOYYRKQR0aeJaDcRDRLRz4moQ3r/F0TUS0Sj7mz7FOm9HxLRt4joD0SUAfBK1/L4OBE97+7zP0QUc7cPzhirbuu+/0ki6iGiI0T0fnf2tybkHgjA1wB8njH2PcbYKGPMZow9yBj7gLvN9UT0Y2kf32zSnb1+kYg2AsgC+AQRbQqc5x+J6A73cZSIvkpEB4ioj4i+TUTxKX4dIKILiOhJ9/N4kogukN57DxHtcS2evUT0Tvf1Na71M0pEA0T0P1WOze/5Gvcz7SGij0vvV/0tSPu+j4gOAPjTOPfxRQAXAbiJiMaI6Cb39fVEdA8RDRHRDiL6K2mfsN/T64noGSJKEdFBIrpeOs1D7v8j7jnOD1pb43yeDxDR54loo/uZ3k1EC8b5ihQAwBhT/2bhPwD7ALw65PWPAHgMziw6CuA7AG6T3v9bAM3uezcAeFZ674cARgG8HM4kIeae5wkA3QA6AGwDcK27/SUADgWuqdq2lwPoBXAKgASAHwNgANaE3MN6970Tatz/9QB+LD1f5e5juM8fAHDAPZ8BoBVAGsBaaZ8nAbzdffx1AHe4190M4LcAviRtOwLgwirX8h4AD4e83gFgGMDV7jX8tfu8E0ASQArASe62SwCc4j6+DcC/SN9BtfPye77NPd5pAPr576LWb0Ha90fuvvGQ4we/3wcAvF96ngRwEMB73fs7C8AAgA01fk+XuNepATgdQB+AN4Z9h8HPttbnKV3fbgDrAMTd518+3n+rs+GfsgjmHtcC+BfG2CHGWAHOgPkWPlNmjP2AMZaW3juDiFql/X/DGNvInBl43n3tRsbYEcbYEJwB8swa56+27V8BuJUxtoUxlnXPXY1O9/+eem+6Cj90z2cxxkYB/AbO4AEiWgtHcO5wLZBrAPwjY2yIMZYG8K8A3s4PxBhrY4xNNA7wegAvMsb+272G2wBsh+fesgGcSkRxxlgPY2yL+3oJwEoA3YyxfB3n/RxjLMMYewHArfweMc5vweV6d9/cBO8NAK4EsI8xdqt7f88A+BWAt0rb+H5PjLEHGGMvuM+fhyNiF9d5vvE+T8D5je107+fnqP1bVbgoIZh7rATwayIaIaIROLPyMoBFRKQT0ZddV0EKzgweAGTz+WDIMXulx1kATTXOX23b7sCxw87DGXT/X1Jjm3oInuOn8AbJdwD4X1eUuuBYKU9Jn9ud7utToRvA/sBr+wEsZYxlALwNzmDdQ0S/J6L17jafBEAAniCiLUT0t+OcR77P/e55gRq/hSr7TpSVAF7Kj++e450AFlc7PhG9lIjuJyf4Pwrn/ut131T9PKXnE/mtKlyUEMw9DgJ4nTuD5f9ijLHDcAa/qwC8Go6rZJW7D0n7N6odbQ8cFwVneY1td8C5jzfX2CYDZ/DmLA7ZJngv9wDoIqIz4QjCT93XBwDk4Lhm+GfWyhib6iByBM5gKbMCwGEAYIzdxRh7DRzB2w7gu+7rvYyxDzDGugH8HYCbw2IpEvJnucI9L1D7t8CZyPcd3PYggAcDx29ijP19jX1+CscFt5wx1grg2/B+f+NdS83PUzF5lBDMbkwiikn/DDh/WF8kopUAQERdRHSVu30zgAKcGXcCjvvjWPFzAO8lopOJKAHgs9U2ZIwxAP8E4LNE9F4ianEDnxcS0S3uZs8CeAURrXBdW/883gUwxkpwMpG+AsfffI/7ug1nEP46ES0EACJaSkSXTeD+KPBdxAD8AcA6InoHERlE9DYAGwD8jogWEdFVRJSE852MwXEVgYjeSkRcNIfhDJB2jXN/logS5AT+3wuAB5dr/RYmQx+AE6Xnv3Pv72oiMt1/5xHRyTWO0QxgiDGWJ6KXwJmccPrh3OeJoXvW+DwnfUcKAEoIZjt/gDOT5f+uB/ANODOuu4koDSdY+FJ3+x/BMaUPA9jqvndMYIz9EcCNAO4HsEs6d6HK9r+E4zr5WzgzwT4AX4Dj5wdj7B44A97zAJ5C/YPBT+FYRL9gjFnS65/i1+W6ze4FIGok3CyWi2oc9wL4v4scnEDplQA+Bkd8PwngSsbYAJy/vX9y720Ijp+cz6TPA/A4EY3B+S4/whjbU+PcD7rXfh+ArzLGeAFWrd/CZPgGnBjDMBHd6MZSXgsnlnIEjlvm3+AEpqvxQQD/z72e6+BMEAAArpvuiwA2uq6ml8k7MsYGUf3zVEwBciZfCsWxxZ01bgYQDQzIijoholUA9gIw1WeomArKIlAcM4joL8nJ12+HM3P8rRrAFIrjjxICxbHk7wAchZPrXYbnClEoFMcR5RpSKBSKeY6yCBQKhWKe0/BGU9PNggUL2KpVq473ZSgUCsWs4qmnnhpgjIUWSc46IVi1ahU2bdo0/oYKhUKhEBBRsCpboFxDCoVCMc9RQqBQKBTzHCUECoVCMc9RQqBQKBTzHCUECoVCMc9RQqBQKBTzHCUECoVCMc9RQjBJNu4awN6BzPG+DIVCoZgySggmySd/+TxueWj38b4MhUKhmDJKCCZJvlRGwaq1aJRCoVDMDpQQTJJS2UbZVp1bFQrF7EcJwSSxbAZLCYFCoZgDKCGYJFaZoVxWQqBQKGY/SggmScm2lUWgUCjmBA0TAiJaTkT3E9FWItpCRB+pst0lRPSsu82DjbqeWuzsS+Nr9+xEvau1lW0GxoCyrYLFCoVi9tNIi8AC8DHG2AYALwPwISLaIG9ARG0AbgbwBsbYKQDe2sDrqcrvnjuCG+97EblSua7tS2VHAJRnSKFQzAUaJgSMsR7G2NPu4zSAbQCWBjZ7B4DbGWMH3O2ONup6ajGSKwEASlZ9Izt3CSmLQKFQzAWOSYyAiFYBOAvA44G31gFoJ6IHiOgpInrXsbieIMNZRwiK5foGdsvdzlImgUKhmAM0fKlKImoC8CsAH2WMpULOfw6ASwHEATxKRI8xxnYGjnENgGsAYMWKFdN+jSPZIoD6haBU5haBEgKFQjH7aahFQEQmHBH4CWPs9pBNDgG4izGWYYwNAHgIwBnBjRhjtzDGzmWMndvVFbr28pQYyXLXUJ0WgesSUllDCoViLtDIrCEC8H0A2xhjX6uy2W8AXEhEBhElALwUTizhmDKScyyCUt2uIWURKBSKuUMjXUMvB3A1gBeI6Fn3tc8AWAEAjLFvM8a2EdGdAJ4HYAP4HmNscwOvKZSRjGMR1Ns7yAsWKyFQKBSzn4YJAWPsYQBUx3ZfAfCVRl3HeJTKNtIFSzyuBx4sVkKgUCjmAvO+snjUTR0FgGKdFgEPFlsqfVShUMwB5r0Q8EAx4A3w48EFQFkECoViLqCEwE0dBWq7hgbHCrjuN5tRtGzJIlBCoFAoZj9KCCSLoFaw+NE9g/jRo/vx4tG0ihEoFIo5xbwRgoGxAu7b1od8oJ/QcJ0WQaHkvGfbKmtIoVDMLeaNEDy6exDv+69NFQvOy8HiWkLAq44t2/aazikhUCgUc4B5IwQrOhIAgINDWd/rskVQK2uo4FoSZZuJgjIVI1AoFHOBhvcamiks50IwnAMAXPebzThnZTuGsyVoBNisPougbDOVNaRQKOYU80YI2hMmkhEdB4eyYIzhZ08cxIGhLKwyw8rOJPYOZGoGi3mMoGwzVUegUCjmFPPGNUREWN6RwKHhLIYyRRTLNl7sG8POvjQ2dLcAqF1H4MUIlEWgUCjmFvNGCABgWXsCB4dy6BnNAwAOj+RwNF3AKUIIalgEFl+VjKk21AqFYk4xr4RgeUccB4ayQgg4Jy9uga5RzWAxf69c9oLFNgNsJQYKhWKWM6+EYEVHArlSGZsPj/peX7uoCaZO41gETtaQ7BoCHAtBoVAoZjPzJlgMAMvbncyhJ/cNwdAIhk7QibC0LY6IrtVcoUy4hqT0Uf7c1Bt73QqFQtFI5pUQrF7YBADYtH8Yi1piWNAUgaYRiAgRQ6tdRyDFCGSLQNUSKBSK2c68EoJVnQksbYvj8EgOS1pj+Nc3nSYWTDB1rXYdgbAIbF92UVktYK9QKGY58ypGQES45CRnzePFrTGsW9SMtYuaAQARQ6uZPsotAqsccA2pGIFCoZjlzCshAIBLTloIAFjSGvO9buq1XUNFS2ox4XMNqaIyhUIxu5l3QnDB6k4sa4/jrBXtvtdNN1i8vTcFJs3yC1YZqXwptI4AULUECoVi9jPvhCAZNfDwp16FK05b4ns9YmjY2ZfG5Tf8GQ/s6Bevv/lbj+D06++WYgRMrEcAwOcmUigUitnIvBOCakR0wiG3IZ3cqnrz4RSAQIzAVhaBQqGYOzRMCIhoORHdT0RbiWgLEX2kxrbnEZFFRG9p1PWMR8TQxKDem3Iqj3ulCmReUGYz5ssuUumjCoVittPI9FELwMcYY08TUTOAp4joHsbYVnkjItIB/BuAuxt4LeNi6p4mcgF4Yt+QeI0vaWkFCspslTWkUChmOQ2zCBhjPYyxp93HaQDbACwN2fQfAPwKwNFGXUs9hArB3kHxWjpvAXDbUNsqRqBQKOYOxyRGQESrAJwF4PHA60sB/CWAbx2L66hFxJCEwHUNbetJV2xXUUegXEMKhWKW03AhIKImODP+jzLGUoG3bwDwKcZYzWR8IrqGiDYR0ab+/v5am06aSMAiYIxhOFNE1PB/RJUtJlQdgUKhmN00VAiIyIQjAj9hjN0essm5AH5GRPsAvAXAzUT0xuBGjLFbGGPnMsbO7erqasi1ykJQLNsYyhQxnC2KJS45FS0mlEWgUChmOY3MGiIA3wewjTH2tbBtGGMnMMZWMcZWAfglgA8yxv63UddUC9Mg3/Oe0TxGcyUsa4/7XreCdQRKCBQKxSynkVlDLwdwNYAXiOhZ97XPAFgBAIyxbzfw3BOGB4ujhoaCZWNHbxo2Q4UQlAN1BGphGoVCMdtpmBAwxh4GQONu6G3/nkZdSz3wYPHaRU3YfDiFbT1OOIOvYcApMydYrJGzQpmyCBQKxWxHVRa78BjB6q4m6BphqysEy4JC4Dadi7mr0agYgUKhmO0oIXDhQtCeiGBxS0wsZ9nd5u9SatlO0zkuBMoiUCgUsx0lBC6m6xpqiRlY2h5Hyi0g60hGkIx4a1Ha3CJwty+r9FGFQjHLUULgwoPFLXETS9u8AHFbIoKmmBdK4S0mPNfQsb1OhUKhmG6UELhEhEXgCYGuEVpiBpJRTwjKttN0jm+vCsoUCsVsRwmBS0R3Epxa4o5rCADa4iaICE3RgEVgMxUsVigUc4Z5tXh9LYRrKGYiEXE+lraECQBIRryPyRauIW4RKCFQKBSzGyUELlHDmeG3xE3E3eBweyICAMI1pGsEy7ZRKqv0UYVCMXdQQuBy0boF+KfXrMPJS1rEspRtrhA0RZ1BP2Hqbh0BQ8xQ6aMKhWJuoITApSVm4sOXrgUAxCM6lrbFRQ0BtwjiEV0Ei7lrSLWYUCgUsx0lBFX46Qdeita4EyPgweJk1BDpo1FlESgUijmCEoIqrOxMisdcCOLCNWQjaqqCMoVCMTdQQlAHV525FPGIjnu29qkWEwqFYs6h6gjqYEVnAu+/6EQYOqHklhKLFhM11iw+msqjz132UqFQKGYqSggmgK5pyJccIYjWYRF8/JfP4+/++6ljcm0KhUIxWZQQTACdgGzRaUYXNTR3TYLqQrB3YAzPHxpBKl/CwaEs/rS9b1qv58O3PYO7tvRO6zEVCsX8QwnBBNA1DdliGYBjERiaVtUisG2G3tE8bAZs2jeEV371AfztDzdN6/XcubkXT+wdmtZjKhSK+YcSgglgaIQcFwJDg65R1crigbGCWOT+sT1DQjCms+6AVzkrFArFVFBCMAF0nZCRXEOGRrCqBIsPj+QAOOLx6O5B8XrBmp6BmzEGm0EJgUKhmDJKCCaATgQeEogaOnSdqtYR9Iw62UKXnLQQL7irnQFArlSelmvhlkjRUumrCoViaighmACGRuJx1NSgE1WNERxxLYI3nb3U93p+uoTAVSRlESgUiqnSMCEgouVEdD8RbSWiLUT0kZBt3klEzxPRC0T0CBGd0ajrmQ50WQh0J0ZQLWvoyEgeiYiOV560EKbu7RdmERQte8ICwS0CJQQKhWKqNNIisAB8jDG2AcDLAHyIiDYEttkL4GLG2GkAPg/glgZez5TRAxZBrRjBkZEcutviiEd0nLm8TbzOg80y/98dm/GBH00so0gJgUKhmC4aJgSMsR7G2NPu4zSAbQCWBrZ5hDE27D59DMCyRl3PdOATAhEjCBeCntEclrQ63Uu/9KbT8InLTgIQ7hra3Z/Bnv7MhK5FxAhqVDYrFApFPRyTGAERrQJwFoDHa2z2PgB/rLL/NUS0iYg29ff3T/8F1okvRmBooo4gVyxjy5FR37a9qbwQgjULm/GSEzoAQFQmy6RyJaRypQldi7AIpikLSaFQzF8aLgRE1ATgVwA+yhhLVdnmlXCE4FNh7zPGbmGMncsYO7erq6txFzsOuuZ9XFFDh0bOgPyDjXvx+hsfxp2bewA4qZ3DmRI6klGxPV/IJixGMJItIV2wJrTamWcRKCFQKBRTo6FCQEQmHBH4CWPs9irbnA7gewCuYowNhm0zU9ClT8uJEWiwbFs0lvv4L55HXyqPTLGMYtlGR9IU28cjzs5hQjDqWgOyVbD58KhYKS0MS8UIFArFNNHIrCEC8H0A2xhjX6uyzQoAtwO4mjG2s1HXMl34LQJeWQyM5Z0is7GChf995jCGM0UA3prHAETb6nwgWFywykIcuCDsOprGld98GP9+5/aq1+LVESghUCgUU6OR6xG8HMDVAF4gomfd1z4DYAUAMMa+DeA6AJ0AbnZ0AxZj7NwGXtOUMALBYsMtKBvIFHHGslYQEX79zGG87MROAEBH0hOCOBcCyy8Eo5IVMOI+Pjjk1CDs6EtXvRaVNaRQKKaLhgkBY+xhADTONu8H8P5GXcN0o0lCEHEtAstmGM4W0dUUxcXrunD9b7fi8b2Oh6stxCIIpo+OZj0h4KIwVnAsjOZY9a/HKyibXNZQvlTG5367BR9/7UnobIqOv4NCoZizqMriCcAtAkMj6BrBcJvODY0V0ZGM4lXrFwEA7tt2FIDfIhBCUKpuEfDHadfV1Bw1UY2pWgQ7etO47YmDeHKf6l6qUMx3lBBMAF5HEHVXJ4sYGnKlMgYyRXQ2RdDdFoOuETa7vYU6JItA10hsLzMSahE4/zfVsgimKAQ822iyFoVCoZg7KCGYANwi4KuTLW9P4MW+MRQtG53JCAxdw5LWGDLFMnSNKlw7cVNHIVBHIFsEPGuIB5+bopVCULDKuHdrnwgSTzZYzK/DqtI0T6FQzB/U4vUTIGgRnLAgKfz53A20rD2OQ8M5tCdMX0wBAGKmVhEjGAlzDbnHjJp+nR7NlXDpfzyAgbEi/u/rTwYw+Rl9wQ1aK4tAoVAoi2AChAkBp7OJC0ECgD9QzImbemiMgAhY0BQVgWNuEQQXsbnj2cMYGHNSUzMFPpBP0jXkWhITKWJTKBRzEyUEE0C4htwq4RO7PCHgVcTLXSHoCBGCWJgQZItoiZloT5gVweLgGH9gKCse8zRUy2aTWvWML5BjqfRThWLeo4RgAvCCMu6yWd6RAPf+dEquIQBoT1Zm/MQjekXTuZFcCa1xE61xSQjcYHE50OL60HBOPJZjDZNpM6FcQwqFgqNiBBOAt5jgrqGooWNZewIHhrKSa8gRAjl1lBMzKoVgNFdCW8IRAr6qGRcEvvrZH17oQe9oHodHcljUEkVfqiAGcsBxD/H01HoRFoEKFisU8x4lBBNAWASGN+iuWpDE0XQeiYjzUS7vcFxD7WExgoiOo2kvOMwYw+BYUQjB9l6nktgTAme7D/7kaQCOuKxZ2OQKgTeAT2ZWXxRCoCwChWK+o1xDE4DHCCKG97G95uSFeNX6heL5opYYLl2/EBesXlCxf9zUfVlD1/1mC144PIpTulvRIrmGeG1BcPWzoUwRqzodofELgdP47sGd9bfo9mIESggUivmOEoIJoAWyhgDg6vNX4eZ3niOe6xrh++85DxeurRSCqKmJ9QgYY/jx4/vxF2d04xOXnYTFrTGMFSyMZktSsLhykF7Z6QSoC5KLqWjZ+PaDu/GB/9pUd+CY76+CxQqFQgnBBDBChGAixE0vRpAv2WAMOHlJM3SNRLbRlh5vgRsuBJ1SvGFlFYtgd38GxbItahDGo8Ari6fZNVQq2+gZzY2/oUKhmDEoIZgAeiB9dKLIdQTZojNgJ3iVcocTZObtKQBPCOSU01XcIvAFixn2DThLXY5ki3VdC886mu46gp89cQCX/seDoUtyKhSKmYkSggngtZiYpEUQcYSAMYasGytIuG0kVrhB5hcOe4u4lRlDqWyLbWOmhu42RzBkiyBTtHBo2KkxkHsX1YLvP91trHf0pZEtljGUqU+QFArF8UdlDU2AsBjBRIiZOhhzBmE+y09EHIugNW6iOWrg0d3eIm22zUSV8UcuXYtXrV8oAtVyHcHuo2PgE/vhei0CXpA2zcHiw26tw0i2JERLoVDMbJRFMAGClcUThef6F0reLJ8LARFhWUcCA2MFtMQMLG6JwbIZUnlnhr+iI4EzlreJa5BdQzulBWzqtQiKDaoj4EVvcjM9hUIxs6lLCIgoSUSa+3gdEb3BXY94XhHsNTRRku6gP1a0RIwgbnpG2XK3GO381Z0wdIJtM29tAreTqScE3gC+o29MPK7fIpj+9FHGmCQEldfRM5oTy3gqFIqZQ70j2kMAYkS0FMDdcJag/GGjLmqmotPUYgRtCUc7R7JFUU/ALQLAixO8fM0CZz1kxkRr6pa4sy8Xo7zkGtrZm0ZrnB97YjGC6SwoG86WhMsr7Dqu/fHT+Pzvt07b+RQKxfRQ74hGjLEsgDcBuJkx9lYApzTusmYmhu4WlOmTFQInDXQkW6pwDQHAiV1NACQhsBlSrkXQEnMGeiJnZTTZNdSbymN5RxwtMaMia6hnNBeaGVS0pta9NAwesAb87bU5Q5mCr1+SQqGYGdQtBER0PoB3Avi9+9rkHOWzGK/p3ORunbedGJYsgrgkBG86eyl+9ffnY3VXE3TiQuAMqPIiN7pGPtcQP3Z7MoJhaSaeLVo4/0t/wl/evBEsUKXcCNfQYWmQD4sRFEr2hFxD+VIZ37j3RZWKqlA0mHqF4KMA/hnArxljW4joRAD3N+6yZiZTLShrd11Dw9mSV0cQ8Qb4mKnjnJUdACAsAh4j4K4hADB1rWJlsta4ibZExBcjOJoqAACePzSK7z+817e9t0KZXwhsm1UsnlMvfLYfM7VQ11CxbE8orfSp/cP4+r071brKCkWDqWtEY4w9yBh7A2Ps39yg8QBj7MO19iGi5UR0PxFtJaItRPSRkG2IiG4kol1E9DwRnT3J+zgm8NTN+CQtglYeI8gUkS1VuoZkNCLYUoxAXraSu6hk2hIm2qR+RQAwmHGEQNcIN9z7IvrTBfGeSB8NZA194ffbcPJ1d07KZXR4JIemqIGlbfHQYHGhZGM4W6y/DYZ7jalcfdXSCoVictSbNfRTImohoiSAzQC2EtEnxtnNAvAxxtgGAC8D8CEi2hDY5nUA1rr/rgHwrQld/TFm7cImfPlNp+GVUpO5iRA1dCQiOkZyJeSKZRBVty4M3bMImqOGCBIDnmUi056IoD1h+iyC/rTz+Ia3nYl8qYyb/vSieI+vYRB0Df1go2M5BF1P9ZDKl4RlEuoassqwWf2ppUWLieMqFIrGUa+PYwNjLAXgjQD+COAEOJlDVWGM9TDGnnYfpwFsA7A0sNlVAH7EHB4D0EZESyZyA8cSIsLbX7Jiwr3/Zdpd902mUEbC1EFUOagDjkVQZs4gKMcHAMDQKr82PgCPZCotgvNWdeAlJ3TgBal9BXcNVZv5lyYhBPlSGfGIjra4WeEassq2KHobqjPFlV9bWgmBQtFQ6hUC060beCOAOxhjJQB1RxmJaBWAswA8HnhrKYCD0vNDqBQLENE1RLSJiDb199ffankm0pZwBslcyUI8Ur2w24kR2EjnS774AH8viGMRRJAuWGIAHXTXN+5IOu/JmTyFcdYsLoUUmo1ki9g/mKl6zfmSjbipozVRKQSyhVFvnIDHQZRrSKFoLPUKwXcA7AOQBPAQEa0EkKq5hwsRNQH4FYCPulbFhGGM3cIYO5cxdm5XV9dkDjFj4BZBtlhGMlrdshBZQzmrwiIwq8UIRDDaGWgHxgpojZuIGBpa4qaINwDeIFut+2jYYjffuO9FvPsHT4jnZZvhim/8GXdt6QUA5IplxEwNrYFzyecD6hcCLmjKNaRQNJZ6g8U3MsaWMsaucN04+wG8crz9XCviVwB+whi7PWSTwwCWS8+Xua/NWfhsOVss1ww66xrBtp31i5tj4RaBXM/QljCxdqFTh/D8QccFNDhWFEtotiWcQDJjDIwxqdeQv521eBziGhrNlYSVAQBjeQtbe1J47uAIAKdLaszU0Rb3WybAJC0CLgQNaFfx73dux2u//uC0H1ehmI3UGyxuJaKvcfcMEf0HHOug1j4E4N7bMegAACAASURBVPsAtjHGvlZlszsAvMvNHnoZgFHGWM9EbmC20Z4wRWVxtYwhAKKymLtbZExXAHSNhBi0JSI4Z1U7YqaGh3cNAHAsggXJKAAnhlAqM+RKZVg2E/56OVgsZxWF9SCy3P05/DEP/uZLjrhxyyTlc0X5eyPVkxLKrQieQjud3PzAbuyUWnMoFPOZel1DPwCQBvBX7r8UgFvH2eflcALKryKiZ91/VxDRtUR0rbvNHwDsAbALwHcBfHCiNzDbaHczasYKlq+GIIimESybwSrbFemi3CLQNRJuora4iaih4yUndPqFoNmxCHgLitFcyTc7lwf83lRePOYZOzKWbcOymRigeS3EiCQEMVP3ncs7nneeWzfuw1u//WjVe2eMoWwz4Z6q5RoKFspNFH4PCsV8pt421KsZY2+Wnn+OiJ6ttQNj7GEA4Skx3jYMwIfqvIY5QVsiApsBR1N5dC1trbqdTk5xV6nMKrKEDMkiICKgWBaD70VrFuCLf9iGntEcBjNFdEoWAeC0t5C7p8oFZb2jnhCEZRPxgTlXKiNiaJ5FkOVC4AWLAYQGp+vh1o378N+P7ccbz3TyBqoFi/cOZHDljX/Gv77pNFx1ZkWOQV0MpItY0am6sSvmN/VaBDkiupA/IaKXA1BNYyYBry4+MpofxzWkoWwzWLZdUTdg+CwCDc0xQ4jD2SvbADjVxCPZEhY0+YXAsQg8N43sGuoZRwh4PIFXHvP/+czfiRFoaOPnyoa7hjjBjKU7N/fiyX1D2NGbxv7BDIplt6CsikWw++gYMsUyPvKzZ7G7f2JuHm5V9Y8VxtlSoZj71DsVuhbAj4iIT2GHAby7MZc0t+H9hoDqVcUAoGuAzRisMqtwDclCENE1xKRuqAubYwCAF901Cniw2CcEbg1BxNB8A35fShaCMNeQ8xp3p4hOo24Vca5URiziuYZGpOriMIugVLaha95n8G93bscJC5IwdYLNIBrzpfMWHtk1gDULm7CwJSa2H5PWZ/7TtqNY7Tbtq4e2uInBTBEDASG4d2sfVnYmsHZRc93HUihmO/VmDT3HGDsDwOkATmeMnQXgVQ29sjnKmoXeYCWvRRBE5zECm4ngMIcLg+HGCGRx4RbAtt60+7xSCHg2TlPU8M3K5WyecNeQaxGIdZe9ltO2GztwgsXOOf0WgbPvl950Gq44bbF4LVcs443/uRHPHRxBKlfCYKYoGufx1dnGChbe8b3H8Z5bn/Rdj1xoJlsz9cA/j6AQfPyXz+F7f94btotCMWeZUPc0xlhKqgX4pwZcz5xneUcC3a3OrLaWRaCRszCNVQ5zDWliG1PXxKAGON1MkxEdO4QQuK6hhOeu4RZBIqL7U0arPOZwNxJ3CfGuoOm8hYxrJcRMHS1u3YMvRuCe84xlbTj/xE4ATgC5N5XHswdH8NT+YYzmShjOFEUrbXnGD/iD2QBEi+6lbXH0jPo9lZsPj+LOzdUT0Jq5EKRlq6WMkWwJYyqArJhnTGWpypqBYEV1zlrRDsDfgjqI4aaPlmwm/P/ye4BjGVx9/kq87bzlvvcXNEexd8CpAO50haApYkAjf4ygKWr4gsV+Iah0DZWEa8hvEQBAn9vpNG7qMHQNzVHDV13MrZCIoQkLp1S2hZj0pvKwbOYKgbNfMG10ubtwDyedt2DqhBMWJHEkYBF8/+G9+MyvN1fcQ5D+MW8/XiMxke6rBwaz+OtbHlNFb4pZzVSEYHpXPZ9HnLbMCbX01nBnaG4b6lCLwHUN6UR41/mrcOXp3b73FzRFhcuHu4Y0jdDidiflbppERIdVds7huHaY6LBaM1hc8geLAS++wGseWhP+6uKCu0/U0MQ5ipYnBAcGnUVt0gVLuKjSAYugyxU1zlihhKaogSWtMfQGLIIx9zjV1jLgBXOyRcDrKCaSUvr0gWE8umcQL9aoSTgyksMN9+4cN9VVLeOpOF7UFAIiShNRKuRfGkB3rX0V1XnFWqdNxvKOeNVtdCKU3EZtFcFiKX00DD74RwzN1766LW5iR18at27cBwBIRg1Yto2/vPkRfPNPu1Aq22Jd5XpcQ3JxGRc1voxnW8IUrqEjIznk3YE3akpCULbFMQ4MeaubcSsl2GwumHmUzltojplY0hbH0XTBd838Go+MhCe38foJOUbAH0/EIuDuq1rVz/ds7cMN975YM46xrSeFs79wjwjyKxTHkppZQ4wxlTrRADZ0t+D+j18iFqsPQ9dIFGFVBIulrKEweFxgQTLi627aGjfxxF6vorcpasBmwJ7+MRxY1IxS2UYiYmA4WwpduYw3oguzCHqDFkHcqaDuTxdw8VfuF+6wqK6L+ylatogdHJSEgDOWt6ARRBU0d0Xdt60Pf3ih1xUCxyJgDNjRm8bKzgSaY6aIWfSM5sUSoDL8/sKEIFtFCJ7YO4QN3S0+ceVCUKu1NhewTKG6pdGXyoMxxyoZL2Ppgz95Cm84oxuXnzpjG/UqZhlTcQ0ppsAJC5IVvn8ZeTnKasHiakLA4wILmv2uFH68V6zrwtvPW4517oCTKZZRKtsoWrZohFesYRGExwhcIXAtira4U0Hdl8qjVGbY0+/ELKpZBEE3EOAMsl3SPfCB9N5tR/Grpw9hYKwghAAArvzmw/ib7zkNbrlIHa5iEfD7GxgLcw1VCkGuWMZffedRvPO7j/lez9QhBFzQg8FvGR6TqdYEUObuLX14bI9atU0xfSghmKHoGgn/drVgcTUh6HJdQ53JiO/1fW4L6c+94RR8+c2n+7KWipYtLAJgvIIyr46AXwN3e8SkGMForiQCv0Pu2ggRXUNUr4wRhJEtlrGiI4HvvetcvHbDIiEafMDe059Bc8xEd5tnWT13yGm4JyyCkXB3DBe1sYIlAr1cFOQYQTpfwnce3C0Gen587/16LALnc8sUqt8r/7ytkM9dhjEnpXiyy4kqFGEoIZihOEtVOo+DbadFsHg811AguPrdd52LT79uPU5YkHSP4339xbKNYpkJcQitI7C9FhOAIwiL3Bl7RbDYXZyGt8S2mXMfmkYwpYB0bpyF6SOGhldvWITOpqgYSHk18FjB8lkEAIQFkXW3DaaVinsp2yKNd6ebatsf4hr6wws9+NIft2PTfm8GLscdps8i4AsF1bYI+PsZleKqmEaUEMxQZHdQcMAXFkGV1c24S6gzIAQXre3CtRevFs9lgSmVgxZBSGVxmTeb84LFzTETyYgugsW8yrktbsKymW8g5j2OIj6LwC84wVvi8YRERBcz9QGpS2pLzERzzBSiN5ItgjEmrrGaa6hUtnGq2+uJF99xS6Ng2SLrarfr0uLpsQDw4E5vcaR6gsWeRVB98OZiUW2hIA4Pcs9Ei2DrkRT+58kDk97/ticO4I3/uXEar0hRL0oIZijy4G/WaDoXhmcRRELfF8eRjlsQrqH6s4ayRWdpyvZkRMymuWuo3XVL8dgAABEbCEsfBRxroi2wGhsXgmRER65Uhm0zX7tsvmjPk/9yKa67cgNKZYaBsaKwNKpl6pTKDCs6EmiOGdjR69RIyoFjvv/uo05a6FGpmO3ZAyPicX3BYlcIasziufCGtf8O224mWgQ/33QQ19+xddL77+hN49mDI+OKoWL6UUIwQ9Hkxeqr9BoKvs5Z0ZHA3778BLx2w+Ka55CtjqJlo2TZiBoaDI2quIb8WUN8/YHu1jh4ijx3DS10rZKdUjpkNCgEZb8QtMZNdATiGnzbeMQAY0BfOu8LZHMhICIsbHHOeWDIER+NgJ6RXGj+vmXbiBga1i9uxvYezyLg1gq3PngzO7kPk1x5PJGsoelwDQUb/80kCpbj6ptsa3AumI1YiEhRGyUEMxTZ7VMRLHYFQKviGtI1wnV/sQErOhOh7wePAzgDUbHMYBoaDJ1C00fDsobiER3dbZ6PPiaEwG1+d9QrtBJCILmG5AGtJW6gMxn1LcTDt+XZTPsH/Wmm8upt/Jx7B5xtutviyBTLFXEIxtz23rqGkxY3Y0dfGgWrjHTewjI3pTdbKKNglUV9w9E0t3g0ZKUBfWxCweLxhWC8YDGvscjMQCHg9zCRluMyXDBHlBAcc5QQzFD8rqHgwjTO1xZMK50ovmCx6xqK6E4LiGD6KM9WAbweQ7kSFwIva0cIgTs7l9tEVLiGyjbyUpFYS8zEusVNOE1apyEiYgTOzH+/m/nENVBez5lbIfvc9hqL3U6l8jV8+LZn8NMnDrjHJpy0uAXpvCUqg/l1Z4tl7B/MioA9twi6mqO+YHKmjhhBsY6sIT54WgG3SDpf8llV/Fgz0SLg11YrE6ye/WuJqqIxKCGYoeg1gsXmOOmj9SILTNGtIzB1p7V10DUkD1AiWOyuu7zEFYKIoYlr6khEKoQqLFicK3rnaYmb+NwbTsV/v/8l3jUazjF47IJbBDzzSS7u4oM4T5NdFCIED+w4ikd3DwJwhJCLxaFhJ6jMA+y5kiXiAwBw1A0WdzX5hSA9gRhBPa6hoEVw5Tcfxmu//pB47lkEMy9GIERqkkJQOI5C8MlfPofbnz50zM87U1BCMEPxWQQB15A+TvroZM5R4hZBFdeQ/FzOGkpEdCx1XUMxw7tOTaOK9FXuGpLTR/NWGc3uYN4SM6BrhKihCxGRs4YATwhO6XasBtk1lIgYaIoaIULgb4fNhcHUNXHcwYw30APA/dv78dnfbEYioiNiaGLA72qOikGYMYZMwal+5kV5YRRdq2c4U8T/+enTIu7wp+19+MWmg+KzACotAn6/3O8ezNyaSRSnGL84nhbBXVv68Ig7QZiPKCGYoWhUPVhsjlNZXC+ywBQsW6x9YOrOMpRfvWsH7tzc4yyZKWWz5Et+i4C7hoLdVPkMnRMJiRHki2WxXYuUMcRdTHwf4RoayiCia1jrruvQEvN3SVnYHMV+N0awKOCeYoyhYNliZm7qJISAN5/jRXi3/HkPGANu+8DL0OGur2DqhLZ4RNQo5Epl2Czc8gCAwbECekfzYqb73KFR/O75Hjziril968Z9+NIft4uYBeAEix/fM4iBsYJwgwGeQPDtipY9bjzhWDNVi+B4CkG+VJ60S+tY0chsKiUEMxTZrVK5ZjG3CKb29ckC4w2OGiK6hucPjeKm+3fh2h8/jV8+dShgEVgo286gKscIYmZACJr9hW3CInDPW7Qci6AtEcH6xc3YsKRF7BsMLAuLYCCLruYoNixpQSKi+1YsA5wZO5+9Bwdo4aLxWQSOkHCLgLuGipaN9UuaccbyNhGoTkYNJKK6sAj4Z8bvPziAfe63W/EPtz0tBjiensrXUUjlnQ6ph0dyUh2Bjff+8EncunEvHnpxQByrKGII3uCfnWED11RjBDxYfKyzhvgEIVjTMtM47fq78G93bm/IsZUQzFA0n2uoWkHZ1M4RrCMAIILFvCIYADYfGRWzz4iuIVssi8EwbupoiZloihq+bB8A6HKzeFa4XVZ5jIDIiUMUy0xYFXd+9BV4+0tWiH090fALQbpgYUlrDJeevBBPf/Y1vkV5AG/wlx9z1xBvcMcHcEOTLIIxLgRe+uriFue6eRwiGTGQjBjIFp0USS4oQSH4x/95Fg/u7MdQpoiBsWJFFg1vacEHvBcOjQrXULHsFMONZEu+BoF8kJXTS7M1gs/Hg4JwDU1uQOWupZHssW3Hzb+fsHW1Zwqlsu1k6QX+xqaLhgkBEf2AiI4SUejqIETUSkS/JaLniGgLEb23UdcyG5EH+cr0Ue4amj6LgGPqBEMn36zs8HBOtJdoiRtI5y2cfv3dALwBurstVtUiWOEuKBORYggRQxOVxfKay5xowDWUlILCJ3YlQUQV55PPCVS6hvKBfP6IoUlC4Aw+chEeT4ttct1PiYiOeERH2WYolm2RBcS3G82VkC+V8etnDuOR3QMoWGVkClZo+2y+PeC4jLgQ8Nl0pmD51icYK1j47kN7fP73iaybcCyYcrC4dHxcQ/y8hRlsEfDfTNAVOl000iL4IYDLa7z/IQBb3bWQLwHwH0RUuxR2HqH7XEPB9FHuGpraOcLST013BTHujjxxQRKHR3JV/dF8ML7itCW4eF2X7z3u++dCEA0KQdnxy4YN6EGLQI4/nLCg+iL1clyCu424q6jSItCEwAiLIOntv9jtRZR03UfJqCHWa8gWykgXSr77608XRJO9QskWdRLFoEWQK4Ex5lkEh0fETD8r3E5lX0fWjbsG8MU/bMNje7yA5kwLGAfXtJ4o3CI41kLAJwj5GWwR8N9KS8ACni4aIy8AGGMPEdGqWpsAaCanYX4TgCEAM2uKcxyRZ/sVweJpihEEs5H4axHp9bWLmvDI7kExUP3lWUsxmiuhKWriBxv3ivUOPvrqdRXHWuS6hvgSk1Fp5m/qhJLFxhUCESyWtjmxK1n1nnhRGd+nKWoI1xD/Q+dBN1MnRA0NRN4ylc0xw3Vb2aKZnXANRXUk3MeZoiUsgvWLW6ARcGAwg+FuJ85RsGwULBuZoiWyvDjpvIVssQzLZiACthxJoW2NMwfibpVMwcKYlO3EB0c5BbUeIfjZEwdABLztvBXjbjtVRIxgkgJ1vC2CmRws5u5EOUtuOmmYENTBTQDuAHAEQDOAtzHGQqedRHQNgGsAYMWKxv+gZwLyGB0MFk9fQVnl/hFd872+blEz7trSJ2IGZ61oxxWnLUHZZjhrRRtes2FR1eNfuHYBPv269bj05EXusaWKYUMT6xGE+T29mgPeTsP7DFbXFAJnRp+I6NA0QnPM8ILFAdPf1DUQEZIRQww+iYiBeERHMWdjSasbIxCuIUNYB9liGWOuRdCeMLG0PY69g1nxORUsxxKwWWU2USpfEufrTEYwnC2JpTNzJWfbTNHCWMFCe8LEcLYkjiG7g+qpJfj07S8AAC4/ZQlaE7UHkUPDWXz5j9vx5Tef7qvPqJcpZw0Ji+DYzgeFRTCDXUOp3Ox1DY3HZQCehbPk5ZkAbiKilrANGWO3MMbOZYyd29XVFbbJnENOHw0Gi6eroCzUNaRrPkthjZumyfPZ5bUQ/uKM7tDZPCdm6rj24tVoi5uIGpqvCjiijxcj8LuGZIKL2Mtw1xD3/TtC4LcI5HsFPLcTkdNCgu/LLQLuPmpys4YAd8buWgRNMQOrOpPYN5DxXEOuRQA4Fog8sKbz3hoIC5tjKNtMDJ58lj9WsDCWt0TvJX4PcmuJieTr3xbSFfSnjx/AU/uHxfP7d/Tjd8/34LFJ5tMXp+ga4utaH+usobzUOwsAtvem8JW7tk+6Z1Ij4N9/o1xDx1MI3gvgduawC8BeAOuP4/XMKHwxgmBB2bQJQZhriMQAGTM14f8+4Oa0hw3M46FphJ9d8zK854JV0nk00aQs3DXkDxaHvRcGz1TiaaHNMbOqRcAtH+73j5s6iAjxiI6YqYmMJD6IJyK6zyLg7bBbYiZOWOAIgbAISrYvSCw300vlShh1BYMHtPngx4UgnbeQKZbFfjzlVO5zJPcuKljl0Jbb/Np//uTBive+evcO3+v73dYczx0aqdi2Hqba/uJ4ZQ3lA66hN9y0Ef95/+4Z1c8pNYeF4ACASwGAiBYBOAnAnuN4PTOKWsFiU2QNTY9rSO5dZxoaIgYfIA0sdZuw7eMWwSRzVs9a0S5aUwNODIC7VkKFoIZFUIuWmIGolA3UHDOEXz3oA/YsAm+gB5z77m6Ni/hHk2wR8GBxseysYbykBTFTx6rOJNIFS7TdLlhlX9ooH9AXNEWRzlvCNcQD0iKzyb3GwUDwWlgEUsqoPPP+70f347KvP+QL6pfKXvHcgaFsRUFSoVT29ZTi33FwFbZ6mUodgW27jQA1qlml3QgKIljspvBavHp75oQsuWuoeba5hojoNgCPAjiJiA4R0fuI6Foiutbd5PMALiCiFwDcB+BTjLGBasebb9SXNTQ9lcVyLn5U14SlkIjqWJCMIqJroso1zIqYDBFDEz/umsFiSQju/aeL8cinX1XzuLwdtScEZkVBGccMWgTu/ys6Eji52/NSJqNejIAfdzhTxFMHhnHB6k4AXu+jZw+OiHPJ5+Mppqd0tyBXKmPITQ3lFoyY8buzUD5md7jprPyzkuMCsijs7h/DWMESgxngBV3XL26GZTNfK21+jXJGE2/f/dzBkQm7RRhjU3IN8X158eGxdA9xi6Bo2bDtmVmnkcqXQAQ0RRojBI3MGvrrcd4/AuC1jTr/bKeeNtTVViir+xyukLS5y0oCXvoo4MyMNY3Q1RwVC7wE4xWTJWJo4pg1g8VGZbxiPE5a1CxcQ76soaoWgXOuhOnsc8Pbz4Q8DspZQ1wU/rxrAEXLxgVrHCFY5QrBC4ed2XSh5E8bffmaBfibl67Ejr40HtzZL5rcebUOzjUG3SqdwjXkdx0523qiwD/LfKksrpd/p6d0t2J7bxqHhnOi+M0qOy1F+GzYthn2D2ZFcHr/YFbcUz3IlsXu/jFcddPD+N67zxNLh44HF80FzRH0pvJI5a2KFfYahfy7kDOWZlJjv3TeQnPU8BWaTieqsniGUruyeJp6DbnHaUt4LhtTl1xDkm+cD0RBUZr0uXVNzPpCg8XG5FxDAHDTO87Gv7/ldACOqygVKCjzrsHf2ZQLgvMZeOf1hMCzCO7d2gddI5y3qgMAsKw9Dl0jMfinA51Go4aOC9YsQIub/ndoOAsir8ldoYo7QsQIctw1FJ4+2isJAWc051gdp7jWzeERby0HPnDz8x5NF1CwbLGY0XZ31bZ6kSueH98zhOcOjfraZ48HFyRuEciNAhuN/Jlt6/HuezJ1GluOjOLgUHb8DSdIKldqWHwAUEIwY6mv19DUhCBqaogamm9hGTlYzAe9RNQQ5vNUU1Y5EV0TA3SoRWD66wgmQszUhbupOWagaDmB27D0UcArGEtEwoPQvNdQIqILSyNXKuOkRc0ir9vUNbGoDeC5csT9uPfBfbyHhnNojhrieJygW8XLGvK7jmKm5gtmehaBd4/cItjAhWDYCybz7bgQ8I6t56xs9+1bL7L1wxvk1Wq7XW1/zzU0/r6MMTz84sCUm7HJ7rTNR7z4SK2FhKrx4duewX/cvWNK1xNGKl9qWA0BoIRgxqLVChZPk0UQM3X8/sMX4W9eulK8FpFiBCJbRhqoJzNDD0Me4GtlDU3VFcX/eNJ5q8IiMIKuoSpCsG5RMy47ZRHOW9UBXSNhwWzo9mc7r+r0XCnBGS2/Xz6rOzScQ0vcrLCGgrnsQggK/hhBa9wUrqFcsSxcGvLslg/mS1pj6ExGfFlFfAbOB2AeAzpjeZuz7wR99MHqaWBiwdZCQAjqsQieOTiCv/n+49i4a2qhxYL0mW0+PDWLoC9VaEhBXCpnNayGAFBCMGPh/n+NUOEX5AIwHbPzNQubfDMNU9ekxWA83zhnsllDQeQgcL3B4snAZ+DpvBViEfhdYPEqgbhk1MB3rj5X+Nf5YC13SwW8gDFQGZgOWgS9qTxa42bNOgzA3/ICgIhdtMZNYRH0SkFgOWWVD+Zt8QiWtcdFXAKQ+uu417mtJ424qWN1VxKmThMezMKEYKxQxuU3PBSaulptf97rKViEF8aOXsf1NNWlLeXvSl5adaIWQb5UxljBmnQdRS1SeeUampeIwT60DcT0uIY48uzcNLSKdYJl94U5jVlDHJ5CKRNsMTFZlrqD9/7BTGWMwL0X7ppKTLCzY6VFUL3QTVgEkujWJQRN4e23mmOmCCz3jFa6fABgNFsEkSM+S9vjPtdQQaRKOsd4Yu8Qzl7ZBkPX0BqPTNw1VK4c/IYzRWzvTWPT/qGQPfwELYJUHRbBLnfQzk0xqCtbUX2pvPjNTNQiGHQzwRqxjGg6b/l+O9ONEoIZCh/kg+sVy+9NlxDI7hdTJ+EakoPFnGmzCNyBUSNvsJbh7pqpCsH6xc5gvb03XWkRGH7BCy6sMx4nBywCnmUT5s7iri75j7kjGQkNlMu0JyqFgC+ow91EPFAMBFxDuRJa4yY0jbC0LY5DIzmRFsoth4JlYzRXwrbelAh8t8YNEWiuFz6Qy40FuaXSM5oP3Ydj20y4ZzqSERB56bS14Cu9TXXglT+zoUxRxMwmmjXEaz8a0QwwlSs1rIYAOL69hhQ1qG0RTE+MgONrDy25hpKRSotguoSA30N3Wzx0sH/dqU4/o8UtldbCRGhNmOhujWF7T6pioOeutWBBWd3HDpjqJy1uhqERVnc1YXuvP2OGD5BN0h/zBy9ZU9MiiJvOMpmGRr4lLA3NKZjrdyube3xC4A8Wt7nXuLA5hqJlI11wZpaeRWDj6f3DYAx4iSsEbYnIpF1DLXFTXBcXqLCKZ5l33/qEqHGIuY0C66kj4BbBVBfoCbrxFrbEoNHE6wi4RTDdQmDbDGNFq6GuISUEMxTeayhsdskFQJtiHQHH5xqSuo+GxQim2zW0soo7pSMZwbvOXzUt51q/pAXbe9MVPn0va6h2sDjIZ65YH9qgbElrHPd//BLc8dwRbO/1Z45wIdA1wqdftx7nrmzHhu6Wmn5oLhoRQ4MlDS6ORWDUZxG4FgUXrdFsyRECKUbwxL4hGBrhrBXtYttg8dl4CCGIGRVC0DOSB2NMVGoH2X10DEfcbaOGhhapCLAauaLXUmOy3U45wfqStriJpPT5yvD6izAB5x1sp7uLabpggbHGNZwDlGtoxuIFhCu/osUtMXzolavxqvULp+Vc0UBXUD5ANkkVtZzpDhavqNFAbrpYv7gZu46OIZW3JBH1PmOvjqC+P7RrXrEaH750beh7yzsSNbOgAODai1fjXHf2Xcsi4J9/NGAxOUts6lKMII92t7OoHAcZzRaFRdASd47Ffe9y1tCREafQjFtMbXFzwhYBryOQrSQuJrlSWcQcDg1n8ZW7tvsqeOU004jbnHC8rKE9A2MicD7VGXhQ1NsSJhJRPdQi+Pq9O/Hmbz0SepxGuYYavRYBoIRgxmLUiANoGuETl60XWSxThbuC+Hn5fnSKNgAAIABJREFUYJ+IVs6Upyt9lJvjvNVzI1m/pAWWzbCtJyVmVfJ9JCbpGqqGPHDzSXC1WIeuUajVR+QJQXBfg8cI3IGqL5UX8Qk+qPWnC+hLFdCW4ELgWgQ5rzuq838ZuWLZd++tCVM0xePkS2W8WKNAjAeLZSEYlFZYO+IGtO/d2of/vH+3eM4Y8wkBtwjGCxbvdns6AZPvdsrJl8o+/3tbPFLVItg/mMWuo2OhLThEsLhUhm0zbDkyig/91FmzOlu0Jt3NVDScUxbB/KOWa2i64bPziNufv1ah1XQVlPHZYljG0HSzbpHTmuLwSE4MVH4hmJhraDxkIeAz8uCsXiYW0k2VrwMNhAiBpiERMcSA0zOaFzUMTrO7Ml751QfQm8rjpMXNALwBOpXzWwR8vQQ5ftIaN5EuWL4Gdv/1yD5c+c2HQ90ev9h0EH9+ccB3nmTgszwywq0Dt/JaNNmzIdeDeRZBbdfQrqNj0Ajoao6GBot3HR3D2295tK56hIJlC8EEHCFMRPXQmb2z9Kgd+h53DfFjPrCjH79/vgcbdw3g7M/fg/u2HR33WsLwlqlUFsG8o1aweLoxdA0aeaLjpY8avv/l65oqPOVxyTEQAjkryRMC7z6WtMVhaIRl7dPjpopK7h6e9VMr+ykWIkCdyYhw5wRrKXjWEODMFgfGCiLWki85aymPFSx87DXr8PcXrwYgC0FlS+6RXMkngly83nDTRtz8wC4AzipqBcuuCOI+c2AYn/zV87h14z4AwOJWp9XGmkXNvu34981n73xw48t9ciKGhpZ4PRbBGJZ3JNCeMEMtgif2DuGxPUPY1jN+m4t8qeyzZFrjphODCYnf8NqNoUxlVtVgpiAeZ4sWjrqTnZ9vOoh8ycZTB4Yr9qkH5Rqax/C//emagY9HxNBEOmUkkFaZED14qGrAb6J86vL1OH1Zq2hp0EiaY6Yw/fkfkyywS9vieOa610zbtfgsgkQdFoHppdLy/7/wxlNFHCISsBhMXRNLZu511xDobo0jamgolMpiJt/RFBHfVzXXEODEEuKmJ/Z8JbOtPSnc8ewRAF6qppzWaZVtfObXm30N+t567jL87h8uxHKp3YapkxfYFUJQ2Vbb+Zx0NLsB53+47RnsOho+kO8+OobVXU2Im+Ezd74uxKHh8fv+5C3bJwROsLi6RQD43V4c2SLIFss46gbN79vuWAIv9o1V7FMPKWURzF/4cpTT5ZMfD3llsovXdeGzV27AyW4OPvehT1cLasBZn+CO/3NhRa+dRsGtguaYAY0qZ9nT2cdFHvTbExFoVNuy464hXthm6houWLMAp3S3AgiLEWii+I37yhe3xhAzdeSlNQbkDK+miHPfoa0ociWfa6gt7tUubO9N42g6L4QgnS/h8T2D2HJkFDfe9yK29aTE8qCAE9c4eUmLcCsmIzoWt8bQw11DRb9FMBZwAfEYQb5k47fPHcE3/7Sr4vMq2wx7BjJYs7AJ8YgeahHwGbtcRFeNQqmMZMQQk662RASJaHiMICssgkLFe4NjBSHquVJZuD95RhX/DCcKtwgaWUeghGCGwltMTJcrZjyivopiA++78ATR2oJbBNOVMXQ84EIQM5yGdI28FzlDyCkaqx17iAeyloIiVZk1RMJa2+MOLo4QaMiXbFhuBo+cBKBp5HO5yBZBtlj2VVUHXRC/euqwCEI/e3AEb7vlMbz+xodx45924dUnL8IVpy2Rrs3vVmyOmehMRsXAnAtYBMHGdBHdv6Tpys7KVtiHhrMoWjZWdyURN53sqa/fsxMP7uwX2wxzIRinhoF/FnKjwraEaxGEZA1xi2BgzG8R5EtlDGSKwr2YkywCzv7BjIjNlMp23R1WuWgqIZiH8MncsQgWA24hWZVz8T/qY2WdNAKeYcU7rjbS5SbP4N9/0Yn4ylvOqLk9twiEC86oLQSGRkI09gQtAstb3StowbXETMk15B/kfBaB6xpyuqsa+K9H9on3eC+eD1+6Fj9+30tx8zvPFmsmyPfOhao5ZrhrHPiFgLs7ZCEwdYKmkc86C/ua+Cx/eUcCiYiBbNHCdx7ajd8/f0RsMxgQgi/8bivu2dpXeTA4g3jM1MRsXsQIQiwCLgTBGMHPnjiAomXjilOdNt7cNcR/ZyctaobNgH0Djqvqu3/egytu/HPo9QRJ5UtIRvSGxgtn71/2HKdWHUEjkOsHgvAMkGMVr2gEXAjKbjFQI0VNHrjXLWrC609fUmNrr+W2HIuRqQwWa+I72TuQQdzU0Rx1luj0uYYC+7VK9QHBdhuJQNYQAJy9sh2v3bDY19SO99q/8vQluHDtAkQMzbeATDDRwBECr3dRPuAayvhSR7ll5F13WEYQF5NkxEA8omM4W0K+ZCOVs1C0bGQKlhCew8M5HBzK4nsP78W1P36q4liAIwRRQ3f/aYiZOpJu1pCc8mnbTFQxy0JQsMq4+YHdeMkJHXilW9vTm8qhaNki7nTVWd0AvGro/QNZHHEL7caj0WsRAEoIZixe1tCxGXzlGEEQecGW2QrvHzOUKSJaQ/SmAz6zjBpaXcF17pKImeGfM59li/5Tuia+k/1DGSxsiYKI3BiB5xqKGP5zt8ZNKX3ULwTymhAdiQjeeGY33nLOMlz3Fxt823EhaJMGpo6kvLCRvz1Jc8xEWyJSYRFwt0g6UEwGAPJyxWEBW+6mikd0xE1dDMqpfAn/fud2/NV3HhWvHRrJ4c7NvQCAk5c0VxyLH49bBNwaSkQMlG2GQ8Nef6ZcqSwC43Jg+OBQFkfTBbzt3OUi5sVn/n/9khX4xtvPxLvPXwUiYIe74E+6UELZXad5PJy1CBobS5u9f9lzHJ28P/pjQcTQqqY4imDxLI4RcItgYKzoWgSNjxHUyhSSEd1PeaO9KkLABwNDJxGMzZds0bEzZjjB4mquIZ9FUMM1pGmEG95+Fs5Z2Y7WuIlHPv0q/PqDF4AIopW1vKod75AakYRPtgg6kiayRae+IZg+yoPF3KIBgKvO7MbnrzoFXc3RKkJQFvcrWzKpfAl7BzLY2pNCf9oJ3BYtGz95fL9znckotvWkcN82z0XEmLNcJ48R8EA5F7KL/v1+POTWSMiuIjlYPOqm5HY2RcT1HHAFc3FrDFeduRTJqIHTlrbi0T2DvvsPdsQNsxAa3XkUUEIwY5nONQfqISIFi8OuJWY21q/eaPhg6bgBtIb6W70W2vUVqMUqXEMBIdADQuA2nePw5S6jpoa8ZYtZZvA4LXFDDFpBi6BW9lZ3WxxnrWhHU8SAZTMkI7pv0sBjBNGQam1uEQBOEzwva8hbelMjf/NBU9dw9fmr0BIzkCtV+um5mDizeEkIco5LiDHn/njW1b5BZ1DOFi3c8tAefPKXz4t9SmUGm8F1BxloT7rpvtJxnz0w4uwvBY9l15Co/I2bQlD5im+LpKaJF61dgKcPjCCdL3nLp0pCxxjD2255DP/vt1t999votQgA1XRuxqIdY9fQqd2tNTOUkhFjVruGVnYkcM0rTsRbzlmGO549ckyyhuq1CPhgxgPAZsClw2MIzqww5xSUSUV+C5oj4jj96YKwCIJWT4vrGipads0YQTWaYwbSBctnDQDe4jmyOPiDxc72w9mimM2P5kq46U8vYu9gBk1RAx3JCAbG/Fk2TiC4ukUQNSstAvl7vWjtAvSO5vGaDYuwu38MA2NFpHIlDGaKboBYF0VgLXET1125QbQEkesKdvQ57hwe2E5GdF8dgSj4ipnCutvn1nfIqbUXre3Cf96/G4/sHhRCKKe+bjmSwhN7hyqK9lI5C2u6GjtUN+zoRPQDAFcCOMoYO7XKNpcAuAGACWCAMXZxo65ntmEc42Dx598Y+hUJ4pHGplw2Gk0jfOaKkwEAH7/spIaeiw/c0XHWGuAIITDDa0ciujeo8vdln35XU0wcR3YNhQWLi2Ub6/7vH9EadwYtb3ZdjxCYwGhezJo5LXEnB98vBK5FEDVEQ7zhTEmc7/lDo3janWl3t8Zw+amLRddSTlwq6npi7xBueWgPWmIGVi9scq9Z87m0UrkS5F/o+sUt2PjpVwEAPvKzZ3BgKCtcMj2jeZywICnahZ+0qBmnLm0V+152ymLc/Y+vwFfv2iG24deyvCMhXD+AVPAVN8T3MpwtoSlq+Kryz17RjmREx8MvDgiXmNzw7uebnJXcdvePoVS2xfeXbvB6xUBjXUM/BHB5tTeJqA3AzQDewBg7BcBbG3gtsw7ea2imDL5Owc3stQiOJXLvpnqIiRhBeJquFyPgVdHkWztZWASGU0dQzTUk1wqMBhY6qdciAPwFZwBAROhIRnznS0a8GIHnGioK15C8vkJTzMC7L1hVIdByh9VvPbAL927rw+3PHEa+VAa5RYGyINrMGYA58upuTsuIsghO85YX290WFLwnE0fXCOsWNWP9khbsG8ggXyqLDKdl7Qlki2VhmcgWgSZ9L/LSpYDzPa5Z2IT9kiDJFsFvnzuC5piBUpkJi4IxhlTeEu1GGkXD/rIZYw8BqLVG3TsA3M4YO+BuP7mOTHMUb4WymTH4JqKNDbDOJTSNENE1n5+5FnzgiI8TLOYBQ/6b4MLBYwTBOoLg9xVs8Cf7nSckBInK2WlnU7Ri+dHmmIF1i5qFBcHTPIPIs2aZhFQ1LLtiRrIlxE0dRFQztiGv7tYU1ZEpWBhzexvxSuftvSl0t8YqFhnirF/s5P/vOjomgsW8dfqglK0UMTTJsnP+X+NaLjKdTVH0jebFfY3lLdz2xAHkimUMZ0t4xbou97o8K6RsszkdLF4HoJ2IHiCip4joXdU2JKJriGgTEW3q7++vttmcQp9hFsFbz1mON5297Hhfxqwhamj1xwh4QZlIHw3ECEKyhgBv8F7QzIXAqSOw7HDX0Gs3LMaDn7gEi1qc7eW2xvUs08ktkrDlM7uao77ZeWvcxAvXX4YL1iwQ2w+OFVAs2xVFYk1VhCBuGsIikNM1hzJFKa5S+RlzYZUL3Xi3Vt5em1sEO3rTFdaADH9vW09KBIuXdzgZaEPuNaVylk9IuK0TKgTJCPYPeS20799xFP98+wu4181kOmt5G3SNsNNt+S0HohvJ8QwWGwDOAXApgDiAR4noMcbYzuCGjLFbANwCAOeee+7kmnrPMjSNQDRzirje8dIVx/sSZhW8grkeTlvWilO6W7DQHaCDA3hUWAR+1xEXAp9FULJFb5vgJELTCCs7k1jcGkdfquAbXOJ1xQic87eHWASfvnx91XUBnNRMTaxCtqAp6mu/UE0IEhEd2SJv8uasrTCSLWEwU0DM/UzkZnmcy09ZjO29ad/gzIPX3J9/ZDSPomVj19ExUQQWxqrOJDqSETy4sx9nuyu4eRZBwT1mySeqvHhudVdle4wFzVGfVcT7EfGYQ1dzFKs6E3hgRz8uWL1AuLfmch3BIQB3McYyjLEBAA8BqF2LP8/QiY5JG2rF9MOrVOvhvFUd+P2HL/JaeRi1YwSmsAhc11CzJwSA19GzWoxisSs4cgCynuZ/fPtg1hAAbOhuqdm9tSMRQa87C+eCx9eJqC0EZWSLFvIlG8vdPj7DmZJkEbhWkRQPuPr8lbjzo68QmXdApfupZySHvQMZWDbD+hoWga4RXn/aEty7rU+IFxcCUchWpfK3mkUgwwPkvEtqS8zEy07sxAuHR/G+/3rSF39oJMdzlPkNgAuJyCCiBICXAth2HK9nxtHZFPn/2zvXILmK6wB/Z2Z2Zndnd7UvPVdCrwgj9EBIAkm8SpFlAYIqjI1jsBNwnCoTyibYLieRww/8I+WSjU05iVMmOKawXRTEMTFQrhhDYmwcUiAIFkIYMBLGhWQZCfQASeix0smP233najSzO7M7szN77/mqtvZOz507fW7f6dPnnO7T4fx3Y3yRz6XL+r7L4YPxxR34gmldzJ/axYBL7ZyJWASduUzYKXrF46c5lpvu63eFy2fT4XTJylxD5WMEw9HdnmWXswgmdQaxiksXBHl5OsqMdtuyaY4OnuStd4MOd7qT/+1Dx8L4i7eKoluellJU+SJFt+vAkXCuf3FQt5irlkzjyPGTPPirnbSkhckTCqvUwSmCEh11qYR5xb9nP2X2jb2BkuxszfD3H1zIZ9fO4/CxE6EVVS6GUSvqOX30PmA10C8iO4DbCKaJoqp3qupLIvIIsAU4Cfyrqm6tV33GIz+++eK6zxYw6sPXP7Kk6rbzI/3iGMGymb385JaL+R+3wrXFjXQ7chkmdhU6Fq8QvCIoF1/yi5wGT6rLT3SyomBxV+gaOr2jHY6efAtbdhwACDfRWf2+STzx6lssikzbjOLdVTv2B6PlGa6z33f4GGc4P70/Z2ZfPpyO2l2i04wq5c7WDLsOHAnTZQy3b/bSM3oY6G5j5/736G5voTOXoSUtkWDxIGeU6PRLKeLoTCYoZDF9w1sEbS2ISLhh03aXm6g3X/09r4a69TKqel0F59wO3F6vOox3JnaaNTBeWTS9dOc2FH6kX24k711E/rxb1s4LdxyDgiLwi5XKWwRBJ7P/8DGy6RSDJ7SixYIF11D1o9OJHblwyuSFc/u5Ztl0FkybwEOfvrDsZ7xy8mktvEXgEwdCwZKJdualRs/RrTPnT+li0+t72bLjAJ2tmWFH26mUsOasSXz/qd+RSaXC6bKFYPGpMYLHv7C65JaeUFh85/EpP37vsqR2Fbnf/B4GxQqk1pgD2jCahEwkqVwpCoogOG/BtAmsmtsXvu9ny/iAaLnreItg3+Hj5FrSFbmFAC74oz6uPW8G86d2VXR+lIHIjmXt2XSY/mEo/EprrwhmRLYS9Yqgtz3LNcum84GzJweustZMybha1CJYMacXgJ+9vJuZfe0VJQZc4wLK3pXTm8/x9qFjbp7/qTGC2f35sveov0yH7td+ePebtwC27zlELpOqKJg/GkwRGEaT4BVBueR/wy1U89NQDx4J8veUSxniO6N9ziKoxC0EgW9/44cXV7QKuZiB7kgnXuH3FSyCwG0yPaJMwu09U8LXPnIOCwcm0NXaUtZt5WcNAayYHSjPg0cHh3ULeVbO6TvldV8+y553j7B9zyGOn6h8nn9Pib0bPOlUYS9qPzPrt28dpDefrdkWseUwRWAYTULBNVT6Rx9aBGU6+KhraChXjx+d/+mKmeRaUmOyXWjUIqh0dNsWcQ21pIVJkQRurSUS+nW1Zcq6raIyntHbHu5YN6NCRdCWTXPpgslcv2omELTF8zsOsPaOX4TfXQkt6SDVdWtLis6iyQRdrZmww/euoSPHT44oJlMtFok0jCahECwu3YnnimIExfhR8sGjg0MqgvZshtc3XgEE+W3qPdqEwlahULki8Avsdu57j9589hTLpdSq7XmTOsu6uYqDxQumdbFz/3vM7B16xlCUf/mz5eHx+6Z08rOXC8kQqpne2ZfP8s6RwSH3zS6330O9MIvAMJqE9DAxgr6OLLP62jlzcul57wWLYLDidCDFGTzrxSmKoGLXUNB579z/Hr35HC3pwp4ZpZTJP113Ll/98OKS14oGiztaM2GCuUpdQ8Xc8v55PLlhTbhKuxp3WV9Hjs7WzGn3IWpVZNKpMABtisAwEkTLMDGA9myGn//1H3P+7N6S73uLIVAElf20z5/VU/Z6taQtmw47tEo7zWhH6RdieaXVWiKzayolpywii5JJp9w2lMHudGvOmsS8SR0smFZ94Dv4/jQD3W18+epFAMwrsXisHFcsmsqVi6aeJkOxVeHv11goAnMNGUaTUJg1NDJXTXQdQaULkG694uzhT6oRA91t7D10rHLXUEQRzOoPRu75bIb9h4+PKGCdz2XCrL4LBybw2OdHn/X+/fMn89qX15dVQKW44YJZAOFuZZ7iNBLd7Vl4+/CYxAjMIjCMJsGP4otTTFRKdP+DZswUO9DdRjolFdctqgjWzp8MFKyEUhbBcORz6brk7KlGCUQJp8C6EX+xReBnDvXm67uqGEwRGEbTMLEzx7KZPWVX2g5HdJTcjDmq5k/tYuqE1oqD01HXkF8vkQ8VwQgsgmym7snbqsHL4OMnxfmKekLXUP0XljbPXTGMhNPakuaBmy4Y+ecjUyqbcVvRm1bP5RMXzqr4/GisxG//2TYKRTCpq7XiRIBjQVtEEbyw88BpSsq7hIp3hKsHpggMIya0pIWUBDt1ZZvQNZTNpMouliuFiPDNj53L4oHusMwnjxuJIrjjT84JYwTNgFcE07xFYMFiwzBGi4jQ2hKkbm5G19BIuHLxtFNehxbBCEb2zZbJ18c5zpzcQT6bDvdi9szqy5PLpJjS1Vrq4zXFFIFhxAivCJoxWFwLRmMRNBs+1cZATxubb1t32orxyxdOYcWcNSXTatcaUwSGESP8SLkZYwS1oD038hhBs+FdQ/lcpmR7pVIyZlZMPJ8Ww0govoOMrSIYxfTRZsO3VXHOoUYw/u+mYRghuVARxNM15NNO1Dst81jgp8J21nkbykpovCoyDKNm+JFyXILFxbSPYvpos7F+0VRSKWHKhPoHg4cjnk+LYSQUv5agXL6i8c6ymUFupDjs3tfXkePjK2Y2uhqAWQSGEStCi2CEaQ+ancXTu/nBjasaXY3YEc9hg2EklDBY3EQraI3mx54Ww4gRXhHE1TVk1Ie6PS0icreI7BaRrcOcd56IDIrINfWqi2Ekhbi7hoz6UM9hwz3AZUOdICJp4CvAo3Wsh2EkBp+czVxDRjXU7WlR1SeAvcOcdjPwALB7mPMMw6iAMEZgFoFRBQ0bNojIAHA18K0Kzv2UiDwrIs/u2bOn/pUzjHFKLuYpJoz60Min5RvA36rqyeFOVNW7VHW5qi6fOHHiGFTNMMYnNmvIGAmNXEewHLjf7VbUD6wXkUFVfbCBdTKMcY0Fi42R0DBFoKqz/bGI3AP82JSAYYyOcPqoWQRGFdRNEYjIfcBqoF9EdgC3AS0Aqnpnvb7XMJJMwSIwRWBUTt0UgapeV8W5n6hXPQwjSfhcQ3HNPmrUBxs2GEaMMNeQMRLsaTGMGJEz15AxAuxpMYwY0RbzjWmM+mCKwDBixIJpE7jxkjmsnNvX6KoY4wjbj8AwYkQ2k+KL6+c3uhrGOMMsAsMwjIRjisAwDCPhmCIwDMNIOKYIDMMwEo4pAsMwjIRjisAwDCPhmCIwDMNIOKYIDMMwEo6oaqPrUBUisgf43Qg+2g+8VePqNBtJkBGSIafJGA+aScaZqlpyi8dxpwhGiog8q6rLG12PepIEGSEZcpqM8WC8yGiuIcMwjIRjisAwDCPhJEkR3NXoCowBSZARkiGnyRgPxoWMiYkRGIZhGKVJkkVgGIZhlMAUgWEYRsJJhCIQkctE5BUR2SYiGxpdn2oRkddF5AUR2Swiz7qyXhF5TERedf97XLmIyD86WbeIyNLIdW5w578qIjc0Sh5Xl7tFZLeIbI2U1UwmEVnm7tk299kx37uxjIxfEpGdri03i8j6yHtfdPV9RUQujZSXfH5FZLaIPO3K/01EsmMnXViHGSLyuIj8WkReFJFbXHls2nIIGePTlqoa6z8gDWwH5gBZ4Hng7EbXq0oZXgf6i8q+CmxwxxuAr7jj9cBPAAFWAk+78l7gNfe/xx33NFCmS4ClwNZ6yARscueK++zlTSLjl4AvlDj3bPds5oDZ7plND/X8Aj8ArnXHdwI3NUDGqcBSd9wJ/MbJEpu2HELG2LRlEiyC84Ftqvqaqh4D7geuanCdasFVwHfd8XeBD0bKv6cBTwHdIjIVuBR4TFX3quo+4DHgsrGutEdVnwD2FhXXRCb3XpeqPqXBL+t7kWuNGWVkLMdVwP2qelRVfwtsI3h2Sz6/blS8Bvih+3z0fo0ZqrpLVZ9zx+8CLwEDxKgth5CxHOOuLZOgCAaANyKvdzB0IzYjCjwqIv8nIp9yZZNVdZc7/gMw2R2Xk3c83IdayTTgjovLm4XPOLfI3d5lQvUy9gH7VXWwqLxhiMgs4FzgaWLalkUyQkzaMgmKIA5cpKpLgcuBT4vIJdE33UgpVvOA4yiT41vAXGAJsAv4emOrUxtEpAN4APisqr4TfS8ubVlCxti0ZRIUwU5gRuT1dFc2blDVne7/buBHBCbmm85sxv3f7U4vJ+94uA+1kmmnOy4ubziq+qaqnlDVk8C3CdoSqpfxbQK3SqaofMwRkRaCDvJeVf0PVxyrtiwlY5zaMgmK4BlgnovKZ4FrgYcbXKeKEZG8iHT6Y2AdsJVABj+z4gbgIXf8MHC9m52xEjjgTPSfAutEpMeZsOtcWTNRE5nce++IyErnf70+cq2G4jtHx9UEbQmBjNeKSE5EZgPzCIKkJZ9fN8p+HLjGfT56v8YMd3+/A7ykqndE3opNW5aTMVZtOZaR6Ub9EcxU+A1BxP7WRtenyrrPIZhd8Dzwoq8/gV/xv4FXgf8Cel25AP/sZH0BWB651icJAlfbgD9vsFz3EZjTxwl8on9RS5mA5QQ/zO3AN3Gr6JtAxu87GbYQdBhTI+ff6ur7CpGZMeWeX/dsbHKy/zuQa4CMFxG4fbYAm93f+ji15RAyxqYtLcWEYRhGwkmCa8gwDMMYAlMEhmEYCccUgWEYRsIxRWAYhpFwTBEYhmEkHFMERuIQkYPu/ywR+ViNr/13Ra//t5bXN4x6YIrASDKzgKoUQWT1ZzlOUQSqekGVdTKMMccUgZFkNgIXu1zynxORtIjcLiLPuERiNwKIyGoR+aWIPAz82pU96JIAvugTAYrIRqDNXe9eV+atD3HX3ipBbv2PRq79cxH5oYi8LCL3upWsiMhGCXLgbxGRr4353TESw3CjG8OIMxsI8slfCeA69AOqep6I5IAnReRRd+5SYKEGaYUBPqmqe0WkDXhGRB5Q1Q0i8hlVXVLiuz5EkJzsHKDffeYJ9965wALg98CTwIUi8hJB2oKzVFVFpLvm0huGwywCwyiwjiAPzmaCNMN9BHliADZVFbFhAAABP0lEQVRFlADAX4nI88BTBInE5jE0FwH3aZCk7E3gF8B5kWvv0CB52WYCl9UB4AjwHRH5EHB41NIZRhlMERhGAQFuVtUl7m+2qnqL4FB4kshqYC2wSlXPAX4FtI7ie49Gjk8AGQ1y059PsFnJlcAjo7i+YQyJKQIjybxLsPWg56fATS7lMCJypsv4WswEYJ+qHhaRswi2UfQc958v4pfAR10cYiLBNpabylXM5b6foKr/CXyOwKVkGHXBYgRGktkCnHAunnuAfyBwyzznArZ7KL1l4CPAXzo//isE7iHPXcAWEXlOVT8eKf8RsIogi6wCf6Oqf3CKpBSdwEMi0kpgqXx+ZCIaxvBY9lHDMIyEY64hwzCMhGOKwDAMI+GYIjAMw0g4pggMwzASjikCwzCMhGOKwDAMI+GYIjAMw0g4/w86fyi4zzel5wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDwpmHTlSkrA"
      },
      "source": [
        "###NOT WORKING (NOT MANDATORY)\n",
        "'''\n",
        "# Validatiob loss of the final iteration from the model traning\n",
        "\n",
        "text_field = data.Field(sequential=True,            # this field consists of a sequence\n",
        "                        tokenize=tokenize_headline, # how to split sequences into words\n",
        "                        include_lengths=True,       # to track the length of sequences, for batching\n",
        "                        batch_first=True,           # similar to batch_first=True used in nn.RNN demonstrated in lecture\n",
        "                        use_vocab=True)             # to turn each character into an integer index\n",
        "valid_data = data.TabularDataset(path=valid_path,                # data file path\n",
        "                                 format=\"tsv\",                   # fields are separated by a tab\n",
        "                                 fields=[('title', text_field)]) # list of fields (we have only one)\n",
        "\n",
        "valid_iter = data.BucketIterator(valid_data, \n",
        "                                 batch_size=64, \n",
        "                                 sort_key=lambda x: len(x.title), # to minimize padding\n",
        "                                 repeat=False)\n",
        "\n",
        "\n",
        "\n",
        "val_loss = 0\n",
        "for it, ((xs, lengths), _) in enumerate(valid_iter):\n",
        "    zs = model(xs)\n",
        "    loss = criterion(zs,xs).sum()\n",
        "    val_loss += float(loss)\n",
        "print(\"Validation loss: \"+ str(val_loss))'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQot60XU4KUT"
      },
      "source": [
        "### Part (c) -- 7%\n",
        "\n",
        "This model requires many epochs (>50) to train, and is quite slow without using a GPU.\n",
        "You can train a model yourself, or you can load the model weights that we have trained,\n",
        "and available on the course website (AE_RNN_model.pk).\n",
        "\n",
        "Assuming that your `AutoEncoder` is set up correctly, the following code should run without\n",
        "error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaUUhd-Q4KUU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1817c74e-372a-49a9-8529-b79de722e987"
      },
      "source": [
        "model = AutoEncoder(10000, 128, 128)\n",
        "checkpoint_path = '/content/gdrive/My Drive/Intro_to_Deep_Learning/HW4/AE_RNN_model.pk' # Update me\n",
        "model.load_state_dict(torch.load(checkpoint_path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG-xazc24KUU"
      },
      "source": [
        "Then, repeat your code from Q2(d), for `train_data[10].title`\n",
        "with temperature settings 0.7, 0.9, and 1.5.\n",
        "Explain why we generally don't want the temperature setting to\n",
        "be too **small**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXF5iBHX4KUU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d00b7edb-6b01-4c9b-d716-4f5e552eb346"
      },
      "source": [
        "# Include the generated sequences and explanation in your PDF report.\n",
        "\n",
        "headline = train_data[10].title\n",
        "input_seq = torch.Tensor([vocab.stoi[w] for w in headline]).unsqueeze(0).long()\n",
        "\n",
        "# ...\n",
        "\n",
        "h = model.encode(input_seq)\n",
        "print(\"Input: \", end =\" \")\n",
        "print(headline[1:-1])\n",
        "tm_set = (0.7, 0.9, 1.5)\n",
        "for tm in tm_set:\n",
        "  print(\"Temperature = \"+str(tm)+\":\")\n",
        "  for i in range(5):\n",
        "    gen_sq = sample_sequence(model=model, hidden=h, max_len=20, temperature=tm)\n",
        "    print(str(i+1) +\") \", end =\" \")\n",
        "    print(gen_sq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:  ['wall', 'street', 'rises', ',', 'limps', 'across', 'the', 'finish', 'line', 'of', 'a', 'turbulent', 'year']\n",
            "Temperature = 0.7:\n",
            "1)  ['wall', 'street', 'rises', ',', 'limps', 'die', 'win', 'at', '$', '<pad>', 'highway', 'election', 'surge']\n",
            "2)  ['wall', 'street', 'rises', ',', 'limps', 'die', 'win', 'at', '$', '<pad>', 'highway', 'a', 'thyssenkrupp']\n",
            "3)  ['wall', 'street', 'rises', ',', 'limps', 'die', 'win', 'at', 'of', 'sciences', '<pad>', 'presidential', 'rebuff']\n",
            "4)  ['wall', 'street', 'rises', ',', 'limps', 'die', 'win', 'at', '$', 'yemen', '<pad>', 'major', 'and']\n",
            "5)  ['wall', 'street', 'rises', ',', 'limps', 'die', 'win', 'at', '$', '<pad>', 'highway', 'a', 'thyssenkrupp']\n",
            "Temperature = 0.9:\n",
            "1)  ['wall', 'street', 'rises', ',', 'limps', 'die', 'win', 'at', 'of', 'sciences', 'but', 'presidential', 'flare']\n",
            "2)  ['wall', 'street', 'rises', ',', 'limps', 'die', 'win', 'at', 'of', 'sciences', 'election', 'four']\n",
            "3)  ['wall', 'street', 'rises', ',', 'limps', 'die', 'win', 'at', ',', 'exit', 'holds', 'disperse', 'best']\n",
            "4)  ['wall', 'street', 'rises', ',', 'limps', 'die', 'win', 'at', '$', 'day', 'yemen', 'for', 'forest']\n",
            "5)  ['wall', 'street', 'rises', ',', 'limps', 'die', 'win', 'at', '$', 'election', 'dams', '_num_', 'stokes']\n",
            "Temperature = 1.5:\n",
            "1)  ['wall', 'flooded', 'stocks', 'leads', 'as', 'daytona', 'radiation', 'british', ',', 'holds', 'starts', 'change']\n",
            "2)  ['wall', 'street', 'rises', 'cows', 'markets', 'party', 'small', 'shoulder', 'will', 'fall', 'after', 'exit', 'facility']\n",
            "3)  ['wall', 'street', 'ice', 'resort', 'bencic', 'sanctions', 'borrowers', 'to', '<unk>', 'over', 'doubles', 'yemen', '-sources']\n",
            "4)  ['wall', 'premier', 'value', ',', 'lyon', 'cheer', 'die', ',', \"'s\", 'largely', ':', 'licenses', 'player']\n",
            "5)  ['wall', 'street', 'rises', ',', 'limps', 'heist', 'courts', 'takeover', 'from', '<pad>', 'people', 'blackouts', 'optimism']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgjhZHJDRljU"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Setting the temperature to be too small make the varience of the sampled distribution too small, so the words samples will be with hige chance the mean, or set of small words that near the mean. As a result all the sample sequences will be very close to thr input sequence - as we can see in tha case of Temperature = 0.7. When we incresed the Temperature value to be 0.9, the begining of all the sample sequences are the same, but near the and (in the 9 word) some sequences sample other words but is was not so diverse. Just when the sample_sequences sample diffrent word (happen in small probability) the rest of the sequences start אo be different from the previous sequences.\n",
        "Temperature = 1.5 contains the largest different word combination in all sequences.\n",
        "\n",
        "**updated (Vova):**\n",
        "\n",
        "By setting the temperature to small value, make the variance of the sampled distribution too small. Therefore, the generation of the words samples will be with high chance of the mean, or alternatively a short set of words that close to the mean.\n",
        "\n",
        "As a result, all of the sampled sequences were almost similar to the input sequence - as shown in this case of Temperature = 0.7.\n",
        "\n",
        "After increasing the Temperature value to 0.9, the sampled sequences were still starting with the same word, however, towards the end of the sequence (starting with the 9th word) some sequences showed different words, yet, those words had lack of diversity. After the \"sample_sequences\" function sampled different word (this tends to happen in small probability), the rest of sequences starting to become different from the previous generated sequences.\n",
        "\n",
        "Temperature = 1.5 contains the largest different word combination in all of the sequences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mftjD-To4KUU"
      },
      "source": [
        "## Question 4. Latent space manipulations (20%)\n",
        "\n",
        "In parts 2-3, we've explored the decoder portion of the autoencoder. In this section,\n",
        "let's explore the **encoder**. In particular, the encoder RNN gives us \n",
        "embeddings of news headlines!\n",
        "\n",
        "First, let's load the **validation** data set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08HhRGfm4KUU"
      },
      "source": [
        "valid_data = data.TabularDataset(\n",
        "    path=valid_path,                # data file path\n",
        "    format=\"tsv\",                   # fields are separated by a tab\n",
        "    fields=[('title', text_field)]) # list of fields (we have only one)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utBRC0tq4KUV"
      },
      "source": [
        "### Part (a) -- 4%\n",
        "\n",
        "Compute the embeddings of every item in the validation set. Then, store the\n",
        "result in a single PyTorch tensor of shape `[19046, 128]`, since there are\n",
        "19,046 headlines in the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GmdVIPB4KUV"
      },
      "source": [
        "# Write your code here\n",
        "# Show that your resulting PyTorch tensor has shape `[19046, 128]`\n",
        "\n",
        "encode_seq = torch.zeros([19046, 128])\n",
        "for i in range(19046):\n",
        "  #print(valid_data[i].title)\n",
        "  headline =  valid_data[i].title\n",
        "  input_seq = torch.Tensor([vocab.stoi[w] for w in headline]).long().unsqueeze(0)\n",
        "  encode_seq[i,:] = model.encode(input_seq)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkyjlUhrW0Ud",
        "outputId": "4f4bcd00-77c4-420d-8174-e8edc9c7c82d"
      },
      "source": [
        "print(encode_seq.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([19046, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlKbT2vW4KUV"
      },
      "source": [
        "### Part (b) -- 4%\n",
        "\n",
        "Find the 5 closest headlines to the headline `valid_data[13]`. Use the\n",
        "cosine similarity to determine closeness. (Hint: You can use code from assignment 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AMt2HIz4KUV"
      },
      "source": [
        "# Write your code here. Make sure to include the actual 5 closest headlines.\n",
        "norms = torch.linalg.norm(encode_seq, axis=1)\n",
        "encode_seq_norm = (encode_seq.T / norms).T\n",
        "similarities = torch.matmul(encode_seq_norm, encode_seq_norm.T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K6ACrrfXZIm",
        "outputId": "172783a5-3872-45f7-ac35-468f69d5fe0d"
      },
      "source": [
        "print('Headline: ')\n",
        "print(valid_data[13].title)\n",
        "similaritie = torch.matmul(encode_seq_norm[13], encode_seq_norm.T)\n",
        "\n",
        "print('\\n5 closest headlines:')\n",
        "max_list = []\n",
        "similaritie[torch.argmax(similaritie)]=0\n",
        "for i in range(0,5):\n",
        "  max_list.append(torch.argmax(similaritie))\n",
        "  similaritie[max_list[i]]=0\n",
        "  print(str(i+1) +\")\", end =\" \")\n",
        "  print(valid_data[torch.argmax(similaritie)].title)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Headline: \n",
            "['<bos>', 'asia', 'takes', 'heart', 'from', 'new', 'year', 'gains', 'in', 'u.s.', 'stock', 'futures', '<eos>']\n",
            "\n",
            "5 closest headlines:\n",
            "1) ['<bos>', 'saudi', ',', 'russia', 'look', 'to', 'seal', 'deeper', 'output', 'cuts', 'with', 'oil', 'producers', '<eos>']\n",
            "2) ['<bos>', 'eu', 'orders', 'quarantine', 'for', 'staff', 'who', 'traveled', 'to', 'northern', 'italy', '<eos>']\n",
            "3) ['<bos>', 'update', '_num_-italy', \"'s\", 'prime', 'minister', 'says', 'new', 'government', 'will', 'bicker', 'less', '<eos>']\n",
            "4) ['<bos>', 'portugal', \"'s\", 'moura', 'pays', 'tribute', 'to', 'cod', 'fishermen', 'at', 'milan', 'fashion', 'close', '<eos>']\n",
            "5) ['<bos>', 'nigeria', \"'s\", 'president', 'and', 'main', 'rival', 'confident', 'as', 'polls', 'close', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAzZt4bU4KUW"
      },
      "source": [
        "### Part (c) -- 4%\n",
        "\n",
        "Find the 5 closest headlines to another headline of your choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkKR09Ce4KUW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4199101-d66e-4858-9174-d9958dfd4a48"
      },
      "source": [
        "# Write your code here. \n",
        "# Make sure to include the original headline and the 5 closest headlines.\n",
        "\n",
        "print('Headline: ')\n",
        "print(valid_data[22].title)\n",
        "similaritie = torch.matmul(encode_seq_norm[22], encode_seq_norm.T)\n",
        "\n",
        "print('\\n5 closest headlines:')\n",
        "max_list = []\n",
        "similaritie[torch.argmax(similaritie)]=0\n",
        "for i in range(0,5):\n",
        "  max_list.append(torch.argmax(similaritie))\n",
        "  print(str(i+1) +\")\", end =\" \")\n",
        "  print(valid_data[torch.argmax(similaritie)].title)\n",
        "  similaritie[max_list[i]]=0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Headline: \n",
            "['<bos>', 'several', 'killed', 'in', 'train', 'accident', 'on', 'bridge', 'in', 'denmark', '-', 'police', '<eos>']\n",
            "\n",
            "5 closest headlines:\n",
            "1) ['<bos>', 'eight', 'killed', 'in', 'quake', ',', 'aftershocks', 'in', 'philippines', ',', '_num_', 'injured', ':', 'agency', '<eos>']\n",
            "2) ['<bos>', 'four', 'killed', 'in', 'train', 'blast', 'in', 'pakistan', \"'s\", 'resource-rich', 'baluchistan', '<eos>']\n",
            "3) ['<bos>', 'three', 'killed', ',', 'including', 'opposition', 'politician', ',', 'in', 'blast', 'in', 'pakistan', \"'s\", 'balochistan', 'province', '<eos>']\n",
            "4) ['<bos>', 'three', 'killed', 'in', 'turkish', 'border', 'town', 'in', 'mortar', 'fire', 'from', 'syria', ':', 'sources', '<eos>']\n",
            "5) ['<bos>', 'sixth', 'person', 'dies', 'in', 'coronavirus', 'outbreak', 'in', 'italy', '-', 'state', 'broadcaster', 'rai', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IgiEpGrRw3k",
        "outputId": "eb07efb9-e896-48f9-b4ed-bbba18ba9f23"
      },
      "source": [
        "max_list[4].long().unsqueeze(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18680])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftUooZD-4KUW"
      },
      "source": [
        "### Part (d) -- 8%\n",
        "\n",
        "Choose two headlines from the validation set, and find their embeddings.\n",
        "We will **interpolate** between the two embeddings like we did in the example presented in class for training autoencoders on MNIST.\n",
        "\n",
        "Find 3 points, equally spaced between the embeddings of your headlines.\n",
        "If we let $e_0$ be the embedding of your first headline and $e_4$ be\n",
        "the embedding of your second headline, your three points should be:\n",
        "\n",
        "\\begin{align*}\n",
        "e_1 &=  0.75 e_0 + 0.25 e_4 \\\\\n",
        "e_2 &=  0.50 e_0 + 0.50 e_4 \\\\\n",
        "e_3 &=  0.25 e_0 + 0.75 e_4 \\\\\n",
        "\\end{align*}\n",
        "\n",
        "Decode each of $e_1$, $e_2$ and $e_3$ five times, with a temperature setting\n",
        "that shows some variation in the generated sequences. Try to get a logical and cool sentence (this might be hard)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SYa3PNc4KUW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1b44c97-0bb8-476c-cd3b-96566afa9913"
      },
      "source": [
        "# Write your code here. Include your generated sequences.\n",
        "n1 = 22\n",
        "n2 = 61\n",
        "print('Original haedlines:')\n",
        "print(\"1) \", end =\" \")\n",
        "print(valid_data[n1].title)\n",
        "print(\"2) \", end =\" \")\n",
        "print(valid_data[n2].title)\n",
        "emb_n1 = encode_seq[n1,:]\n",
        "emb_n2 = encode_seq[n2,:]\n",
        "\n",
        "e0 = emb_n1\n",
        "e4 = emb_n2\n",
        "\n",
        "e1 = 0.75*e0 + 0.25*e4\n",
        "e2 = 0.50*e0 + 0.50*e4\n",
        "e3 = 0.25*e0 + 0.75*e4\n",
        "e = [e1, e2, e3]\n",
        "\n",
        "for k in range(3):\n",
        "  print()\n",
        "  print('Interpolation of e'+ str(k+1)+':')\n",
        "  h = e[k].view(1,1,-1)\n",
        "  tm_set = (0.9, 1.5, 5)\n",
        "  for tm in tm_set:\n",
        "    print(\"Temperature = \"+str(tm)+\":\")\n",
        "    for i in range(2):\n",
        "      gen_sq = sample_sequence(model=model, hidden=h, max_len=20, temperature=tm)\n",
        "      print(str(i+1) +\") \", end =\" \")\n",
        "      print(gen_sq)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original haedlines:\n",
            "1)  ['<bos>', 'several', 'killed', 'in', 'train', 'accident', 'on', 'bridge', 'in', 'denmark', '-', 'police', '<eos>']\n",
            "2)  ['<bos>', 'qualcomm', 'posts', 'court', 'ordered', 'bonds', 'to', 'stop', 'iphone', 'sales', 'in', 'germany', '<eos>']\n",
            "\n",
            "Interpolation of e1:\n",
            "Temperature = 0.9:\n",
            "1)  ['several', 'killed', 'update', 'seeking', 'train', 'take', ',', 'using', 'days', 'office', 'chief']\n",
            "2)  ['press', 'teens', 'robot', 'in', 'blast', 'united', ',', 'in', 'elfa', '<unk>', 'russian']\n",
            "Temperature = 1.5:\n",
            "1)  ['shelling', 'victim', 'to', 'train', 'in', 'trapped', 'vs', 'trump', 'village', '-', 'recession']\n",
            "2)  ['dozens', '...', \"'s\", 'checks', 'woods', 'from', '_num_-google', 'oppose', 'local', '-', 'shares']\n",
            "Temperature = 5:\n",
            "1)  ['accounts', 'come', 'product', 'melbourne', 'clearing', 'kantar', 'devices', 'national', 'save', 'says', 'deficit', 'cup', 'courts', 'gmt', \"'s\", 'crisis', '_num_-wework']\n",
            "2)  ['pursuing', 'tie-up', 'alarms', 'stranded', 'reported', 'surpass', 'another', 'invited', 'boom', 'calls', 'ivanka', 'subscription', \"'s\", 'profits', 'commodity', 'future', 'community']\n",
            "\n",
            "Interpolation of e2:\n",
            "Temperature = 0.9:\n",
            "1)  ['units', \"'s\", 'grizzlies', 'at', 'bombing', 'thousands', 'gets', 'commission', 'its', 'death', 'pepper']\n",
            "2)  ['fees', 'air', 'injured', 'in', 'ship', 'to', 'boys', 'capital', 'british', 'decades', 'to']\n",
            "Temperature = 1.5:\n",
            "1)  ['1mdb', 'five', \"'s\", 'tass', 'ferry', 'take', 'for', 'japan', 'daily', 'death', 'clues']\n",
            "2)  ['explodes', 'canada', 'manager', 'flight', 'ban', ',', 'axel', 'of', 'fear', 'fire', 'co']\n",
            "Temperature = 5:\n",
            "1)  ['blaming', 'putting', 'list', 'blast', 'journalist', 'projects', 'gold', 'northern', 'club', 'lure', 'indian']\n",
            "2)  ['audit', '_num_-two', 'record', 'kenin', 'czechs', 'sunday', 'torch', 'two', 'million', 'export', 'autism', 'kemp', 'ex-minister', 'suspicion', 'norway', 'stocks-trade', 'a.m.', 'di', 'skiing', 'capex']\n",
            "\n",
            "Interpolation of e3:\n",
            "Temperature = 0.9:\n",
            "1)  ['qualcomm', 'grenade', 'chemical', 'to', 'coup', ',', 'projects', 'from', 'field', 'at', 'contract']\n",
            "2)  ['domingo', 'sources', 'sunday', 'cruise', 'in', 'to', 'sfr', 'licensing', 'up', ':', 'airlines']\n",
            "Temperature = 1.5:\n",
            "1)  ['locked', 'update', 'starbucks', 'believed', 'at', 'emergency', 'nyt', 'banks', '_num_', 'petrobras', 'box']\n",
            "2)  ['banco', 'after', 'flight', 'uk', 'truck', 'its', 'contract', 'big', 'income', 'seized', '_num_']\n",
            "Temperature = 5:\n",
            "1)  ['serves', 'messaging', 'posts', 'rages', 'plane', 'bombing', 'industry', 'fails', 'ago', 'volcano', 'passes', 'cyclone', 'time', '_num_-shell', 'mccabe', 'decades', 'britain', 'somali', 'leave', 'snc-lavalin']\n",
            "2)  ['valley', 'scottish', 'flock', 'reaction', 'to', 'frenchman', 'capital', 'months', 'robot', 'double', 'turkey', 'pretax', 'expectations', 'lighthizer', 'verizon', 'st', 'chance', 'coach', 'speech', 'racing']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVyUux6U1O4Y",
        "outputId": "87aa4123-40ae-4328-d32c-7880825c9d9a"
      },
      "source": [
        "n1 = 22\n",
        "#n2 = random.randint(1,len(valid_data)-1)\n",
        "n2 = 18680 #change n2 to 18680, for similar headline\n",
        "\n",
        "print('Original haedlines:')\n",
        "print(\"1) \", end =\" \")\n",
        "print(' '.join(valid_data[n1].title))\n",
        "print(\"2) \", end =\" \")\n",
        "print(' '.join(valid_data[n2].title))\n",
        "emb_n1 = encode_seq[n1,:]\n",
        "emb_n2 = encode_seq[n2,:]\n",
        "\n",
        "e0 = emb_n1\n",
        "e4 = emb_n2\n",
        "\n",
        "e1 = 0.75*e0 + 0.25*e4\n",
        "e2 = 0.50*e0 + 0.50*e4\n",
        "e3 = 0.25*e0 + 0.75*e4\n",
        "e = [e1, e2, e3]\n",
        "\n",
        "for k in range(3):\n",
        "  print()\n",
        "  print('Interpolation of e'+ str(k+1)+':')\n",
        "  h = e[k].view(1,1,-1)\n",
        "  tm_set = (0.8, 1.7, 2.4)\n",
        "  for tm in tm_set:\n",
        "    print(\"\\nTemperature = \"+str(tm)+\":\")\n",
        "    for i in range(5):\n",
        "      gen_sq = sample_sequence(model=model, hidden=h, max_len=15, temperature=tm)\n",
        "      print(str(i+1) +\") \", end =\" \")\n",
        "      print(' '.join(gen_sq))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original haedlines:\n",
            "1)  <bos> several killed in train accident on bridge in denmark - police <eos>\n",
            "2)  <bos> sixth person dies in coronavirus outbreak in italy - state broadcaster rai <eos>\n",
            "\n",
            "Interpolation of e1:\n",
            "\n",
            "Temperature = 0.8:\n",
            "1)  seattle killed sale pistons deter in unrest : reuters rakhine dies\n",
            "2)  several killed , diverted in train , separatist the - sources\n",
            "3)  several killed sale members off including , emergency - <unk> crown\n",
            "4)  several to killed destroyed at british during darfur trump - <unk>\n",
            "5)  several killed , diverted in train , separatist president - <unk>\n",
            "\n",
            "Temperature = 1.7:\n",
            "1)  several to injured dozens roche in , seizure bombings - <unk>\n",
            "2)  somali 5g stolen jan. in fishing from violent parliament : academy\n",
            "3)  vigil since state anti-doping in strongest northern turkish unacceptable game u.n\n",
            "4)  several to state darfur in off who consortium highlights - house trap\n",
            "5)  responsibility bladder killed kabul in muguruza port in - fire spies\n",
            "\n",
            "Temperature = 2.4:\n",
            "1)  feared investment sunday emissions , kills troops sunday l.a. budget walking tops\n",
            "2)  sdf d-day mine checkpoints up shooting stay next york _num_-month rounds\n",
            "3)  sao last attacks police several put , transfers election arrests elderly\n",
            "4)  office _num_-three seven police in semafo northern street much call -\n",
            "5)  jersey kcna supporters sale up eastern stolen days strains at -\n",
            "\n",
            "Interpolation of e2:\n",
            "\n",
            "Temperature = 0.8:\n",
            "1)  last reaches in jolts attacked in solved a city - belgium\n",
            "2)  several to film at many in texas years - show election source\n",
            "3)  last reaches in involved of quarantined fire plane ( broadcaster for\n",
            "4)  this elderly shooting in coronavirus in renewed quake : putin <unk>\n",
            "5)  last reaches : trapped in as remaining still injured state election\n",
            "\n",
            "Temperature = 1.7:\n",
            "1)  horse nine airliner tanker in yoga northern meeting mauritius - york\n",
            "2)  prix : six schools shooting in afraid policemen hits - supreme state\n",
            "3)  air headquarters quarter quarantined in <pad> calls curtailments state the park -\n",
            "4)  sensor e victims rockets in infections northern president the carried _num_\n",
            "5)  contract instagram scenarios tass seven as eastern for shootings year jail\n",
            "\n",
            "Temperature = 2.4:\n",
            "1)  sunday stg seven england fired exxon : setback curbs women york\n",
            "2)  thirteen abbas killed in several corner statement survey - fire member\n",
            "3)  concert in j.c. sale accident as attacks kills pensions <unk> -\n",
            "4)  motorcycling through ! since pool in refinery rohingya month : marketing\n",
            "5)  reaches last u.n achilles nearly in due dead ford suicide zealand _num_\n",
            "\n",
            "Interpolation of e3:\n",
            "\n",
            "Temperature = 0.8:\n",
            "1)  magnitude serena _num_ diverted in saudi lebanon gulf false - canada\n",
            "2)  sixth suspected dies halloween in arabia data in - york dead -\n",
            "3)  sixth suspected dies halloween in iranian from world in - viacom -\n",
            "4)  sixth suspected dies home hold in news iranian high state armed\n",
            "5)  sixth was seven with tanker city including japan - business sentences\n",
            "\n",
            "Temperature = 1.7:\n",
            "1)  carl cancel for dozens in , bpd leaves prison day source\n",
            "2)  magnitude kennedy best workers from wounds uk - cheers moscow times\n",
            "3)  innovation man seven _num_-month passengers : syria arrest december for jimmy\n",
            "4)  6th charged seven government gets clashes japan years fire transition _num_-ftc\n",
            "5)  magnitude predicts college feb border in case subscription state centre paris\n",
            "\n",
            "Temperature = 2.4:\n",
            "1)  comcast headlines 's kingdom contract troops after gig week spain injured\n",
            "2)  cancel millions : across dr all due of israel subscription death\n",
            "3)  bumps civilians to state surge citizen fear of the catastrophic al <unk>\n",
            "4)  automation _num_ _num_-petrobras many in prisoners stop british weakest - statement poll\n",
            "5)  timeline credit gm in quiet ntsb refinery as conditions memorial - moscow\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}